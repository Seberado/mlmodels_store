
  test_pullrequest /home/runner/work/mlmodels/mlmodels/mlmodels/config/test_config.json Namespace(config_file='/home/runner/work/mlmodels/mlmodels/mlmodels/config/test_config.json', config_mode='test', do='test_pullrequest', folder=None, log_file=None, save_folder='ztest/') 

  ml_test --do test_pullrequest 





 ************************************************************************************************************************

 ******** TAG ::  {'github_repo_url': 'https://github.com/arita37/mlmodels/tree/09dbb573cf89ddf861ea945ff13f39f474d48070', 'url_branch_file': 'https://github.com/arita37/mlmodels/blob/dev/', 'repo': 'arita37/mlmodels', 'branch': 'dev', 'sha': '09dbb573cf89ddf861ea945ff13f39f474d48070', 'workflow': 'test_pullrequest'}

 ******** GITHUB_WOKFLOW : https://github.com/arita37/mlmodels/actions?query=workflow%3Atest_pullrequest

 ******** GITHUB_REPO_BRANCH : https://github.com/arita37/mlmodels/tree/dev/

 ******** GITHUB_REPO_URL : https://github.com/arita37/mlmodels/tree/09dbb573cf89ddf861ea945ff13f39f474d48070

 ******** GITHUB_COMMIT_URL : https://github.com/arita37/mlmodels/commit/09dbb573cf89ddf861ea945ff13f39f474d48070

 ******** Click here for Online DEBUGGER : https://gitpod.io/#https://github.com/arita37/mlmodels/tree/09dbb573cf89ddf861ea945ff13f39f474d48070

 ************************************************************************************************************************

  /home/runner/work/mlmodels/mlmodels/pullrequest/ 

  ############Check model ################################ 

  ['/home/runner/work/mlmodels/mlmodels/pullrequest/aa_mycode_test.py'] 

  Used ['/home/runner/work/mlmodels/mlmodels/pullrequest/aa_mycode_test.py'] 

  ########### Run Check ############################## 





 ************************************************************************************************************************

 ******** TAG ::  {'github_repo_url': 'https://github.com/arita37/mlmodels/tree/09dbb573cf89ddf861ea945ff13f39f474d48070', 'url_branch_file': 'https://github.com/arita37/mlmodels/blob/dev/', 'repo': 'arita37/mlmodels', 'branch': 'dev', 'sha': '09dbb573cf89ddf861ea945ff13f39f474d48070', 'workflow': 'test_pullrequest'}

 ******** GITHUB_WOKFLOW : https://github.com/arita37/mlmodels/actions?query=workflow%3Atest_pullrequest

 ******** GITHUB_REPO_BRANCH : https://github.com/arita37/mlmodels/tree/dev/

 ******** GITHUB_REPO_URL : https://github.com/arita37/mlmodels/tree/09dbb573cf89ddf861ea945ff13f39f474d48070

 ******** GITHUB_COMMIT_URL : https://github.com/arita37/mlmodels/commit/09dbb573cf89ddf861ea945ff13f39f474d48070

 ******** Click here for Online DEBUGGER : https://gitpod.io/#https://github.com/arita37/mlmodels/tree/09dbb573cf89ddf861ea945ff13f39f474d48070

 ************************************************************************************************************************





 ************************************************************************************************************************

  test_import 
['model_rank.__init__', 'preprocess.text_torch', 'preprocess.tabular_keras', 'preprocess.text', 'preprocess.ztemp', 'preprocess.text_keras', 'preprocess.__init__', 'preprocess.timeseries', 'preprocess.image', 'preprocess.generic', 'preprocess.generic_old', 'preprocess.tabular', 'model_tf.temporal_fusion_google', 'model_tf.1_lstm', 'model_tf.__init__', 'model_tf.util', 'example.arun_model', 'example.vision_mnist', 'example.benchmark_timeseries_m4', 'example.arun_hyper', 'example.lightgbm_glass', 'example.benchmark_timeseries_m5', 'model_tch.transformer_classifier', 'model_tch.textcnn', 'model_tch.nbeats', 'model_tch.pytorch_vae', 'model_tch.mlp', 'model_tch.__init__', 'model_tch.util_data', 'model_tch.matchzoo_models', 'model_tch.pplm', 'model_tch.transformer_sentence', 'model_tch.torchhub', 'model_tch.util_transformer', 'model_tch.03_nbeats_dataloader', 'model_keras.02_cnn', 'model_keras.charcnn_zhang', 'model_keras.preprocess', 'model_keras.charcnn', 'model_keras.textcnn', 'model_keras.nbeats', 'model_keras.01_deepctr', 'model_keras.namentity_crm_bilstm', 'model_keras.keras_gan', 'model_keras.__init__', 'model_keras.Autokeras', 'model_keras.textvae', 'model_keras.util', 'model_keras.namentity_crm_bilstm_dataloader', 'model_keras.armdn', 'model_gluon.fb_prophet', 'model_gluon.gluonts_model', 'model_gluon.__init__', 'model_gluon.gluon_automl', 'model_gluon.util', 'model_gluon.util_autogluon', 'utils.parse', 'utils.test_dataloader', 'utils.ztest_structure', 'template.00_template_keras', 'template.model_xxx', 'model_dev.__init__', 'model_flow.__init__', 'model_sklearn.model_sklearn', 'model_sklearn.__init__', 'model_sklearn.model_lightgbm']
mlmodels.model_rank.__init__
mlmodels.preprocess.text_torch
mlmodels.preprocess.tabular_keras
mlmodels.preprocess.text

  Error mlmodels.preprocess.ztemp invalid character in identifier (ztemp.py, line 6) 
mlmodels.preprocess.text_keras
mlmodels.preprocess.__init__
mlmodels.preprocess.timeseries
mlmodels.preprocess.image
mlmodels.preprocess.generic
mlmodels.preprocess.generic_old
mlmodels.preprocess.tabular

  Error mlmodels.model_tf.temporal_fusion_google No module named 'mlmodels.mode_tf' 
Using TensorFlow backend.
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
mlmodels.model_tf.1_lstm
mlmodels.model_tf.__init__
mlmodels.model_tf.util
<module 'mlmodels' from '/home/runner/work/mlmodels/mlmodels/mlmodels/__init__.py'>
/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/ardmn.json

  Error mlmodels.example.arun_model [Errno 2] No such file or directory: '/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/ardmn.json' 

  Error mlmodels.example.vision_mnist invalid syntax (vision_mnist.py, line 15) 
mlmodels.example.benchmark_timeseries_m4

  Error mlmodels.example.arun_hyper name 'mlmodels' is not defined 
Deprecaton set to False
/home/runner/work/mlmodels/mlmodels

  Error mlmodels.example.lightgbm_glass [Errno 2] No such file or directory: 'lightgbm_glass.json' 

  Error mlmodels.example.benchmark_timeseries_m5 [Errno 2] File b'./m5-forecasting-accuracy/calendar.csv' does not exist: b'./m5-forecasting-accuracy/calendar.csv' 

  Error mlmodels.model_tch.transformer_classifier No module named 'util_transformer' 
mlmodels.model_tch.textcnn
mlmodels.model_tch.nbeats

  Error mlmodels.model_tchtorch_vae No module named 'mlmodels.model_tchtorch_vae' 
mlmodels.model_tch.mlp
mlmodels.model_tch.__init__

  Error mlmodels.model_tch.util_data [Errno 2] File b'./data/train.csv' does not exist: b'./data/train.csv' 
mlmodels.model_tch.matchzoo_models
mlmodels.model_tch.pplm
mlmodels.model_tch.transformer_sentence
mlmodels.model_tch.torchhub
mlmodels.model_tch.util_transformer

  Error mlmodels.model_tch.03_nbeats_dataloader No module named 'dataloader' 
mlmodels.model_keras.02_cnn
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset
mlmodels.model_keras.charcnn_zhang
mlmodels.model_keras.preprocess
mlmodels.model_keras.charcnn
mlmodels.model_keras.textcnn
mlmodels.model_keras.nbeats
mlmodels.model_keras.01_deepctr
mlmodels.model_keras.namentity_crm_bilstm

  Error mlmodels.model_keras.keras_gan module 'mlmodels.model_keras.raw.keras_gan' has no attribute 'aae' 
mlmodels.model_keras.__init__

  Error mlmodels.model_keras.Autokeras No module named 'autokeras' 
INFO:root:Using CPU
INFO:root:Using CPU
INFO:root:Using CPU
INFO:root:Using CPU
INFO:root:Using CPU
INFO:root:Using CPU
INFO:root:Using CPU
INFO:root:Using CPU
INFO:root:Using CPU
INFO:root:Using CPU
INFO:root:Using CPU
INFO:root:Using CPU
INFO:root:Using CPU
INFO:root:Using CPU
INFO:root:Using CPU
mlmodels.model_keras.textvae
mlmodels.model_keras.util
mlmodels.model_keras.namentity_crm_bilstm_dataloader
mlmodels.model_keras.armdn
mlmodels.model_gluon.fb_prophet
mlmodels.model_gluon.gluonts_model
mlmodels.model_gluon.__init__
mlmodels.model_gluon.gluon_automl
mlmodels.model_gluon.util
mlmodels.model_gluon.util_autogluon
mlmodels.utils.parse
mlmodels.utils.test_dataloader
mlmodels.utils.ztest_structure

  Error mlmodels.template.00_template_keras expected an indented block (00_template_keras.py, line 68) 

  Error mlmodels.template.model_xxx name '__file___' is not defined 
Deprecaton set to False

  {'model_uri': 'model_tf.1_lstm', 'learning_rate': 0.001, 'num_layers': 1, 'size': 6, 'size_layer': 128, 'output_size': 6, 'timestep': 4, 'epoch': 2} {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} {'engine': 'optuna', 'method': 'prune', 'ntrials': 5} {'engine_pars': {'engine': 'optuna', 'method': 'normal', 'ntrials': 2, 'metric_target': 'loss'}, 'learning_rate': {'type': 'log_uniform', 'init': 0.01, 'range': [0.001, 0.1]}, 'num_layers': {'type': 'int', 'init': 2, 'range': [2, 4]}, 'size': {'type': 'int', 'init': 6, 'range': [6, 6]}, 'output_size': {'type': 'int', 'init': 6, 'range': [6, 6]}, 'size_layer': {'type': 'categorical', 'value': [128, 256]}, 'timestep': {'type': 'categorical', 'value': [5]}, 'epoch': {'type': 'categorical', 'value': [2]}} 

  <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> 

  ###### Hyper-optimization through study   ################################## 

  check <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} 
[32m[I 2020-05-22 11:11:26,015][0m Finished trial#0 resulted in value: 7.607574462890625. Current best value is 7.607574462890625 with parameters: {'learning_rate': 0.04695812260455098, 'num_layers': 4, 'size': 6, 'output_size': 6, 'size_layer': 128, 'timestep': 5, 'epoch': 2}.[0m
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  check <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} 
[32m[I 2020-05-22 11:11:28,056][0m Finished trial#1 resulted in value: 5.64333438873291. Current best value is 5.64333438873291 with parameters: {'learning_rate': 0.05197632793585923, 'num_layers': 4, 'size': 6, 'output_size': 6, 'size_layer': 128, 'timestep': 5, 'epoch': 2}.[0m
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

 ################################### Optim, finished ###################################

  ### Save Stats   ########################################################## 

  ### Run Model with best   ################################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Saving     ########################################################### 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/optim_1lstm/', 'model_type': 'model_tf', 'model_uri': 'model_tf-1_lstm'}
Model saved in path: /home/runner/work/mlmodels/mlmodels/mlmodels/ztest/optim_1lstm//model//model.ckpt
sh: 1: ml_mlmodels: not found
mlmodels.model_dev.__init__
mlmodels.model_flow.__init__
mlmodels.model_sklearn.model_sklearn
mlmodels.model_sklearn.__init__
mlmodels.model_sklearn.model_lightgbm





 ************************************************************************************************************************

  python /home/runner/work/mlmodels/mlmodels/pullrequest/aa_mycode_test.py  2>&1 | tee -a  cd log_.txt 
os.getcwd /home/runner/work/mlmodels/mlmodels
############ Your custom code ################################



 python /home/runner/work/mlmodels/mlmodels/mlmodels/optim.py  
Deprecaton set to False

  {'model_uri': 'model_tf.1_lstm', 'learning_rate': 0.001, 'num_layers': 1, 'size': 6, 'size_layer': 128, 'output_size': 6, 'timestep': 4, 'epoch': 2} {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} {'engine': 'optuna', 'method': 'prune', 'ntrials': 5} {'engine_pars': {'engine': 'optuna', 'method': 'normal', 'ntrials': 2, 'metric_target': 'loss'}, 'learning_rate': {'type': 'log_uniform', 'init': 0.01, 'range': [0.001, 0.1]}, 'num_layers': {'type': 'int', 'init': 2, 'range': [2, 4]}, 'size': {'type': 'int', 'init': 6, 'range': [6, 6]}, 'output_size': {'type': 'int', 'init': 6, 'range': [6, 6]}, 'size_layer': {'type': 'categorical', 'value': [128, 256]}, 'timestep': {'type': 'categorical', 'value': [5]}, 'epoch': {'type': 'categorical', 'value': [2]}} 

  <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> 

  ###### Hyper-optimization through study   ################################## 

  check <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} 

  <mlmodels.model_tf.1_lstm.Model object at 0x7f72aa86d470> 
[32m[I 2020-05-22 11:11:35,240][0m Finished trial#0 resulted in value: 0.36679476499557495. Current best value is 0.36679476499557495 with parameters: {'learning_rate': 0.006895422780177583, 'num_layers': 2, 'size': 6, 'output_size': 6, 'size_layer': 128, 'timestep': 5, 'epoch': 2}.[0m
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  check <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} 

  <mlmodels.model_tf.1_lstm.Model object at 0x7f72aa5228d0> 
[32m[I 2020-05-22 11:11:36,446][0m Finished trial#1 resulted in value: 0.46914082765579224. Current best value is 0.36679476499557495 with parameters: {'learning_rate': 0.006895422780177583, 'num_layers': 2, 'size': 6, 'output_size': 6, 'size_layer': 128, 'timestep': 5, 'epoch': 2}.[0m
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

 ################################### Optim, finished ###################################

  ### Save Stats   ########################################################## 

  ### Run Model with best   ################################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Saving     ########################################################### 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/optim_1lstm/', 'model_type': 'model_tf', 'model_uri': 'model_tf-1_lstm'}
Model saved in path: /home/runner/work/mlmodels/mlmodels/mlmodels/ztest/optim_1lstm//model//model.ckpt



 python /home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/textcnn.py    

  #### Loading params   ############################################## 

  #### Path params   ########################################## 

  #### Loading dataset   ############################################# 
Loading data...
Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz

    8192/17464789 [..............................] - ETA: 0s
   24576/17464789 [..............................] - ETA: 48s
   57344/17464789 [..............................] - ETA: 41s
  106496/17464789 [..............................] - ETA: 33s
  212992/17464789 [..............................] - ETA: 22s
  262144/17464789 [..............................] - ETA: 22s
  368640/17464789 [..............................] - ETA: 18s
  475136/17464789 [..............................] - ETA: 17s
  581632/17464789 [..............................] - ETA: 15s
  696320/17464789 [>.............................] - ETA: 14s
  802816/17464789 [>.............................] - ETA: 14s
  925696/17464789 [>.............................] - ETA: 13s
 1048576/17464789 [>.............................] - ETA: 12s
 1155072/17464789 [>.............................] - ETA: 12s
 1277952/17464789 [=>............................] - ETA: 12s
 1392640/17464789 [=>............................] - ETA: 11s
 1515520/17464789 [=>............................] - ETA: 11s
 1605632/17464789 [=>............................] - ETA: 11s
 1695744/17464789 [=>............................] - ETA: 11s
 1777664/17464789 [==>...........................] - ETA: 11s
 1867776/17464789 [==>...........................] - ETA: 11s
 1974272/17464789 [==>...........................] - ETA: 11s
 2039808/17464789 [==>...........................] - ETA: 11s
 2129920/17464789 [==>...........................] - ETA: 11s
 2211840/17464789 [==>...........................] - ETA: 11s
 2301952/17464789 [==>...........................] - ETA: 11s
 2408448/17464789 [===>..........................] - ETA: 11s
 2490368/17464789 [===>..........................] - ETA: 11s
 2547712/17464789 [===>..........................] - ETA: 11s
 2613248/17464789 [===>..........................] - ETA: 11s
 2703360/17464789 [===>..........................] - ETA: 11s
 2809856/17464789 [===>..........................] - ETA: 11s
 2891776/17464789 [===>..........................] - ETA: 11s
 2981888/17464789 [====>.........................] - ETA: 10s
 3063808/17464789 [====>.........................] - ETA: 10s
 3170304/17464789 [====>.........................] - ETA: 10s
 3260416/17464789 [====>.........................] - ETA: 10s
 3342336/17464789 [====>.........................] - ETA: 10s
 3432448/17464789 [====>.........................] - ETA: 10s
 3538944/17464789 [=====>........................] - ETA: 10s
 3620864/17464789 [=====>........................] - ETA: 10s
 3710976/17464789 [=====>........................] - ETA: 10s
 3801088/17464789 [=====>........................] - ETA: 10s
 3883008/17464789 [=====>........................] - ETA: 10s
 3989504/17464789 [=====>........................] - ETA: 10s
 4079616/17464789 [======>.......................] - ETA: 10s
 4161536/17464789 [======>.......................] - ETA: 10s
 4251648/17464789 [======>.......................] - ETA: 9s 
 4358144/17464789 [======>.......................] - ETA: 9s
 4440064/17464789 [======>.......................] - ETA: 9s
 4530176/17464789 [======>.......................] - ETA: 9s
 4620288/17464789 [======>.......................] - ETA: 9s
 4718592/17464789 [=======>......................] - ETA: 9s
 4808704/17464789 [=======>......................] - ETA: 9s
 4898816/17464789 [=======>......................] - ETA: 9s
 4980736/17464789 [=======>......................] - ETA: 9s
 5013504/17464789 [=======>......................] - ETA: 9s
 5103616/17464789 [=======>......................] - ETA: 9s
 5210112/17464789 [=======>......................] - ETA: 9s
 5292032/17464789 [========>.....................] - ETA: 9s
 5382144/17464789 [========>.....................] - ETA: 9s
 5472256/17464789 [========>.....................] - ETA: 9s
 5570560/17464789 [========>.....................] - ETA: 9s
 5660672/17464789 [========>.....................] - ETA: 8s
 5750784/17464789 [========>.....................] - ETA: 8s
 5832704/17464789 [=========>....................] - ETA: 8s
 5939200/17464789 [=========>....................] - ETA: 8s
 6029312/17464789 [=========>....................] - ETA: 8s
 6111232/17464789 [=========>....................] - ETA: 8s
 6201344/17464789 [=========>....................] - ETA: 8s
 6307840/17464789 [=========>....................] - ETA: 8s
 6389760/17464789 [=========>....................] - ETA: 8s
 6479872/17464789 [==========>...................] - ETA: 8s
 6569984/17464789 [==========>...................] - ETA: 8s
 6668288/17464789 [==========>...................] - ETA: 8s
 6758400/17464789 [==========>...................] - ETA: 8s
 6840320/17464789 [==========>...................] - ETA: 8s
 6897664/17464789 [==========>...................] - ETA: 8s
 6963200/17464789 [==========>...................] - ETA: 8s
 7036928/17464789 [===========>..................] - ETA: 7s
 7102464/17464789 [===========>..................] - ETA: 7s
 7192576/17464789 [===========>..................] - ETA: 7s
 7282688/17464789 [===========>..................] - ETA: 7s
 7380992/17464789 [===========>..................] - ETA: 7s
 7471104/17464789 [===========>..................] - ETA: 7s
 7561216/17464789 [===========>..................] - ETA: 7s
 7659520/17464789 [============>.................] - ETA: 7s
 7749632/17464789 [============>.................] - ETA: 7s
 7839744/17464789 [============>.................] - ETA: 7s
 7938048/17464789 [============>.................] - ETA: 7s
 8028160/17464789 [============>.................] - ETA: 7s
 8118272/17464789 [============>.................] - ETA: 7s
 8216576/17464789 [=============>................] - ETA: 7s
 8306688/17464789 [=============>................] - ETA: 6s
 8396800/17464789 [=============>................] - ETA: 6s
 8495104/17464789 [=============>................] - ETA: 6s
 8585216/17464789 [=============>................] - ETA: 6s
 8675328/17464789 [=============>................] - ETA: 6s
 8773632/17464789 [==============>...............] - ETA: 6s
 8863744/17464789 [==============>...............] - ETA: 6s
 8953856/17464789 [==============>...............] - ETA: 6s
 9052160/17464789 [==============>...............] - ETA: 6s
 9142272/17464789 [==============>...............] - ETA: 6s
 9232384/17464789 [==============>...............] - ETA: 6s
 9314304/17464789 [==============>...............] - ETA: 6s
 9420800/17464789 [===============>..............] - ETA: 6s
 9510912/17464789 [===============>..............] - ETA: 6s
 9592832/17464789 [===============>..............] - ETA: 5s
 9699328/17464789 [===============>..............] - ETA: 5s
 9789440/17464789 [===============>..............] - ETA: 5s
 9871360/17464789 [===============>..............] - ETA: 5s
 9977856/17464789 [================>.............] - ETA: 5s
10067968/17464789 [================>.............] - ETA: 5s
10149888/17464789 [================>.............] - ETA: 5s
10256384/17464789 [================>.............] - ETA: 5s
10346496/17464789 [================>.............] - ETA: 5s
10428416/17464789 [================>.............] - ETA: 5s
10534912/17464789 [=================>............] - ETA: 5s
10625024/17464789 [=================>............] - ETA: 5s
10706944/17464789 [=================>............] - ETA: 5s
10813440/17464789 [=================>............] - ETA: 5s
10903552/17464789 [=================>............] - ETA: 4s
11001856/17464789 [=================>............] - ETA: 4s
11091968/17464789 [==================>...........] - ETA: 4s
11198464/17464789 [==================>...........] - ETA: 4s
11280384/17464789 [==================>...........] - ETA: 4s
11386880/17464789 [==================>...........] - ETA: 4s
11476992/17464789 [==================>...........] - ETA: 4s
11583488/17464789 [==================>...........] - ETA: 4s
11681792/17464789 [===================>..........] - ETA: 4s
11788288/17464789 [===================>..........] - ETA: 4s
11894784/17464789 [===================>..........] - ETA: 4s
11976704/17464789 [===================>..........] - ETA: 4s
12083200/17464789 [===================>..........] - ETA: 4s
12189696/17464789 [===================>..........] - ETA: 3s
12296192/17464789 [====================>.........] - ETA: 3s
12410880/17464789 [====================>.........] - ETA: 3s
12517376/17464789 [====================>.........] - ETA: 3s
12623872/17464789 [====================>.........] - ETA: 3s
12730368/17464789 [====================>.........] - ETA: 3s
12836864/17464789 [=====================>........] - ETA: 3s
12951552/17464789 [=====================>........] - ETA: 3s
13058048/17464789 [=====================>........] - ETA: 3s
13164544/17464789 [=====================>........] - ETA: 3s
13287424/17464789 [=====================>........] - ETA: 3s
13393920/17464789 [======================>.......] - ETA: 2s
13508608/17464789 [======================>.......] - ETA: 2s
13615104/17464789 [======================>.......] - ETA: 2s
13737984/17464789 [======================>.......] - ETA: 2s
13844480/17464789 [======================>.......] - ETA: 2s
13967360/17464789 [======================>.......] - ETA: 2s
14090240/17464789 [=======================>......] - ETA: 2s
14188544/17464789 [=======================>......] - ETA: 2s
14311424/17464789 [=======================>......] - ETA: 2s
14434304/17464789 [=======================>......] - ETA: 2s
14557184/17464789 [========================>.....] - ETA: 2s
14680064/17464789 [========================>.....] - ETA: 2s
14802944/17464789 [========================>.....] - ETA: 1s
14925824/17464789 [========================>.....] - ETA: 1s
15040512/17464789 [========================>.....] - ETA: 1s
15163392/17464789 [=========================>....] - ETA: 1s
15286272/17464789 [=========================>....] - ETA: 1s
15409152/17464789 [=========================>....] - ETA: 1s
15548416/17464789 [=========================>....] - ETA: 1s
15671296/17464789 [=========================>....] - ETA: 1s
15794176/17464789 [==========================>...] - ETA: 1s
15933440/17464789 [==========================>...] - ETA: 1s
16056320/17464789 [==========================>...] - ETA: 0s
16179200/17464789 [==========================>...] - ETA: 0s
16318464/17464789 [===========================>..] - ETA: 0s
16433152/17464789 [===========================>..] - ETA: 0s
16572416/17464789 [===========================>..] - ETA: 0s
16695296/17464789 [===========================>..] - ETA: 0s
16834560/17464789 [===========================>..] - ETA: 0s
16973824/17464789 [============================>.] - ETA: 0s
17096704/17464789 [============================>.] - ETA: 0s
17235968/17464789 [============================>.] - ETA: 0s
17375232/17464789 [============================>.] - ETA: 0s
17465344/17464789 [==============================] - 12s 1us/step
Pad sequences (samples x time)...

  #### Model init, fit   ############################################# 
Using TensorFlow backend.
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 40)           0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 40, 50)       250         input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 38, 128)      19328       embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 37, 128)      25728       embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 36, 128)      32128       embedding_1[0][0]                
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_2 (GlobalM (None, 128)          0           conv1d_2[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_3 (GlobalM (None, 128)          0           conv1d_3[0][0]                   
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_1[0][0]     
                                                                 global_max_pooling1d_2[0][0]     
                                                                 global_max_pooling1d_3[0][0]     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            385         concatenate_1[0][0]              
==================================================================================================
Total params: 77,819
Trainable params: 77,819
Non-trainable params: 0
__________________________________________________________________________________________________
Loading data...
Pad sequences (samples x time)...
Train on 25000 samples, validate on 25000 samples
Epoch 1/1

 1000/25000 [>.............................] - ETA: 13s - loss: 7.2680 - accuracy: 0.5260
 2000/25000 [=>............................] - ETA: 9s - loss: 7.4136 - accuracy: 0.5165 
 3000/25000 [==>...........................] - ETA: 7s - loss: 7.3293 - accuracy: 0.5220
 4000/25000 [===>..........................] - ETA: 6s - loss: 7.4558 - accuracy: 0.5138
 5000/25000 [=====>........................] - ETA: 6s - loss: 7.5041 - accuracy: 0.5106
 6000/25000 [======>.......................] - ETA: 5s - loss: 7.5107 - accuracy: 0.5102
 7000/25000 [=======>......................] - ETA: 5s - loss: 7.5637 - accuracy: 0.5067
 8000/25000 [========>.....................] - ETA: 4s - loss: 7.5861 - accuracy: 0.5052
 9000/25000 [=========>....................] - ETA: 4s - loss: 7.6053 - accuracy: 0.5040
10000/25000 [===========>..................] - ETA: 4s - loss: 7.6191 - accuracy: 0.5031
11000/25000 [============>.................] - ETA: 3s - loss: 7.6290 - accuracy: 0.5025
12000/25000 [=============>................] - ETA: 3s - loss: 7.6181 - accuracy: 0.5032
13000/25000 [==============>...............] - ETA: 3s - loss: 7.6230 - accuracy: 0.5028
14000/25000 [===============>..............] - ETA: 3s - loss: 7.6184 - accuracy: 0.5031
15000/25000 [=================>............] - ETA: 2s - loss: 7.6452 - accuracy: 0.5014
16000/25000 [==================>...........] - ETA: 2s - loss: 7.6513 - accuracy: 0.5010
17000/25000 [===================>..........] - ETA: 2s - loss: 7.6756 - accuracy: 0.4994
18000/25000 [====================>.........] - ETA: 1s - loss: 7.6573 - accuracy: 0.5006
19000/25000 [=====================>........] - ETA: 1s - loss: 7.6594 - accuracy: 0.5005
20000/25000 [=======================>......] - ETA: 1s - loss: 7.6751 - accuracy: 0.4994
21000/25000 [========================>.....] - ETA: 1s - loss: 7.6674 - accuracy: 0.5000
22000/25000 [=========================>....] - ETA: 0s - loss: 7.6694 - accuracy: 0.4998
23000/25000 [==========================>...] - ETA: 0s - loss: 7.6593 - accuracy: 0.5005
24000/25000 [===========================>..] - ETA: 0s - loss: 7.6577 - accuracy: 0.5006
25000/25000 [==============================] - 8s 321us/step - loss: 7.6666 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000

  #### save the trained model  ####################################### 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5'}

  #### Predict   ##################################################### 
Loading data...

  #### metrics   ##################################################### 
{}

  #### Plot   ######################################################## 

  #### Save/Load   ################################################### 
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5'}
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5'}
(<mlmodels.util.Model_empty object at 0x7f299e900668>, None)

  #### Module init   ############################################ 

  <module 'mlmodels.model_keras.textcnn' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/textcnn.py'> 

  #### Loading params   ############################################## 

  #### Path params   ########################################## 

  #### Model init   ############################################ 
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 40)           0                                            
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 40, 50)       250         input_2[0][0]                    
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 38, 128)      19328       embedding_2[0][0]                
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 37, 128)      25728       embedding_2[0][0]                
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 36, 128)      32128       embedding_2[0][0]                
__________________________________________________________________________________________________
global_max_pooling1d_4 (GlobalM (None, 128)          0           conv1d_4[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_5 (GlobalM (None, 128)          0           conv1d_5[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_6 (GlobalM (None, 128)          0           conv1d_6[0][0]                   
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 384)          0           global_max_pooling1d_4[0][0]     
                                                                 global_max_pooling1d_5[0][0]     
                                                                 global_max_pooling1d_6[0][0]     
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            385         concatenate_2[0][0]              
==================================================================================================
Total params: 77,819
Trainable params: 77,819
Non-trainable params: 0
__________________________________________________________________________________________________

  <mlmodels.model_keras.textcnn.Model object at 0x7f299d9a8860> 

  #### Fit   ######################################################## 
Loading data...
Pad sequences (samples x time)...
Train on 25000 samples, validate on 25000 samples
Epoch 1/1

 1000/25000 [>.............................] - ETA: 11s - loss: 7.2986 - accuracy: 0.5240
 2000/25000 [=>............................] - ETA: 8s - loss: 7.5976 - accuracy: 0.5045 
 3000/25000 [==>...........................] - ETA: 7s - loss: 7.6155 - accuracy: 0.5033
 4000/25000 [===>..........................] - ETA: 6s - loss: 7.7126 - accuracy: 0.4970
 5000/25000 [=====>........................] - ETA: 5s - loss: 7.6973 - accuracy: 0.4980
 6000/25000 [======>.......................] - ETA: 5s - loss: 7.6513 - accuracy: 0.5010
 7000/25000 [=======>......................] - ETA: 4s - loss: 7.6688 - accuracy: 0.4999
 8000/25000 [========>.....................] - ETA: 4s - loss: 7.6398 - accuracy: 0.5017
 9000/25000 [=========>....................] - ETA: 4s - loss: 7.6666 - accuracy: 0.5000
10000/25000 [===========>..................] - ETA: 3s - loss: 7.6574 - accuracy: 0.5006
11000/25000 [============>.................] - ETA: 3s - loss: 7.6374 - accuracy: 0.5019
12000/25000 [=============>................] - ETA: 3s - loss: 7.6155 - accuracy: 0.5033
13000/25000 [==============>...............] - ETA: 3s - loss: 7.6383 - accuracy: 0.5018
14000/25000 [===============>..............] - ETA: 2s - loss: 7.6491 - accuracy: 0.5011
15000/25000 [=================>............] - ETA: 2s - loss: 7.6452 - accuracy: 0.5014
16000/25000 [==================>...........] - ETA: 2s - loss: 7.6427 - accuracy: 0.5016
17000/25000 [===================>..........] - ETA: 2s - loss: 7.6396 - accuracy: 0.5018
18000/25000 [====================>.........] - ETA: 1s - loss: 7.6445 - accuracy: 0.5014
19000/25000 [=====================>........] - ETA: 1s - loss: 7.6561 - accuracy: 0.5007
20000/25000 [=======================>......] - ETA: 1s - loss: 7.6567 - accuracy: 0.5006
21000/25000 [========================>.....] - ETA: 0s - loss: 7.6768 - accuracy: 0.4993
22000/25000 [=========================>....] - ETA: 0s - loss: 7.6673 - accuracy: 0.5000
23000/25000 [==========================>...] - ETA: 0s - loss: 7.6606 - accuracy: 0.5004
24000/25000 [===========================>..] - ETA: 0s - loss: 7.6679 - accuracy: 0.4999
25000/25000 [==============================] - 7s 290us/step - loss: 7.6666 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000

  #### Predict   #################################################### 
Loading data...
(array([[1.],
       [1.],
       [1.],
       ...,
       [1.],
       [1.],
       [1.]], dtype=float32), None)

  #### Get  metrics   ################################################ 

  #### Save   ######################################################## 

  #### Load   ######################################################## 

  ############ Model preparation   ################################## 

  #### Module init   ############################################ 

  <module 'mlmodels.model_keras.textcnn' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/textcnn.py'> 

  #### Loading params   ############################################## 

  #### Path params   ########################################## 

  #### Model init   ############################################ 
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 40)           0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 40, 50)       250         input_3[0][0]                    
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 38, 128)      19328       embedding_3[0][0]                
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 37, 128)      25728       embedding_3[0][0]                
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 36, 128)      32128       embedding_3[0][0]                
__________________________________________________________________________________________________
global_max_pooling1d_7 (GlobalM (None, 128)          0           conv1d_7[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_8 (GlobalM (None, 128)          0           conv1d_8[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_9 (GlobalM (None, 128)          0           conv1d_9[0][0]                   
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 384)          0           global_max_pooling1d_7[0][0]     
                                                                 global_max_pooling1d_8[0][0]     
                                                                 global_max_pooling1d_9[0][0]     
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            385         concatenate_3[0][0]              
==================================================================================================
Total params: 77,819
Trainable params: 77,819
Non-trainable params: 0
__________________________________________________________________________________________________

  ############ Model fit   ########################################## 
Loading data...
Pad sequences (samples x time)...
Train on 25000 samples, validate on 25000 samples
Epoch 1/1

 1000/25000 [>.............................] - ETA: 11s - loss: 7.4213 - accuracy: 0.5160
 2000/25000 [=>............................] - ETA: 7s - loss: 7.6820 - accuracy: 0.4990 
 3000/25000 [==>...........................] - ETA: 6s - loss: 7.5184 - accuracy: 0.5097
 4000/25000 [===>..........................] - ETA: 6s - loss: 7.6168 - accuracy: 0.5033
 5000/25000 [=====>........................] - ETA: 5s - loss: 7.5869 - accuracy: 0.5052
 6000/25000 [======>.......................] - ETA: 5s - loss: 7.6462 - accuracy: 0.5013
 7000/25000 [=======>......................] - ETA: 4s - loss: 7.6469 - accuracy: 0.5013
 8000/25000 [========>.....................] - ETA: 4s - loss: 7.6264 - accuracy: 0.5026
 9000/25000 [=========>....................] - ETA: 4s - loss: 7.6274 - accuracy: 0.5026
10000/25000 [===========>..................] - ETA: 3s - loss: 7.6053 - accuracy: 0.5040
11000/25000 [============>.................] - ETA: 3s - loss: 7.6137 - accuracy: 0.5035
12000/25000 [=============>................] - ETA: 3s - loss: 7.5772 - accuracy: 0.5058
13000/25000 [==============>...............] - ETA: 2s - loss: 7.5782 - accuracy: 0.5058
14000/25000 [===============>..............] - ETA: 2s - loss: 7.6360 - accuracy: 0.5020
15000/25000 [=================>............] - ETA: 2s - loss: 7.6513 - accuracy: 0.5010
16000/25000 [==================>...........] - ETA: 2s - loss: 7.6695 - accuracy: 0.4998
17000/25000 [===================>..........] - ETA: 1s - loss: 7.6693 - accuracy: 0.4998
18000/25000 [====================>.........] - ETA: 1s - loss: 7.6428 - accuracy: 0.5016
19000/25000 [=====================>........] - ETA: 1s - loss: 7.6392 - accuracy: 0.5018
20000/25000 [=======================>......] - ETA: 1s - loss: 7.6475 - accuracy: 0.5013
21000/25000 [========================>.....] - ETA: 0s - loss: 7.6528 - accuracy: 0.5009
22000/25000 [=========================>....] - ETA: 0s - loss: 7.6422 - accuracy: 0.5016
23000/25000 [==========================>...] - ETA: 0s - loss: 7.6646 - accuracy: 0.5001
24000/25000 [===========================>..] - ETA: 0s - loss: 7.6705 - accuracy: 0.4997
25000/25000 [==============================] - 7s 279us/step - loss: 7.6666 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000
fit success None

  ############ Prediction############################################ 
Loading data...
(array([[1.],
       [1.],
       [1.],
       ...,
       [1.],
       [1.],
       [1.]], dtype=float32), None)

  ############ Save/ Load ############################################ 
/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning:

WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB

/home/runner/work/mlmodels/mlmodels/mlmodels/model_sklearn/model_sklearn.py:1187: DeprecationWarning:

invalid escape sequence \*

