
  test_pullrequest /home/runner/work/mlmodels/mlmodels/mlmodels/config/test_config.json Namespace(config_file='/home/runner/work/mlmodels/mlmodels/mlmodels/config/test_config.json', config_mode='test', do='test_pullrequest', folder=None, log_file=None, save_folder='ztest/') 

  ml_test --do test_pullrequest 





 ************************************************************************************************************************

 ******** TAG ::  {'github_repo_url': 'https://github.com/arita37/mlmodels/tree/14ea42490a753d0445fda074e8d3eac878d2aeed', 'url_branch_file': 'https://github.com/arita37/mlmodels/blob/dev/', 'repo': 'arita37/mlmodels', 'branch': 'dev', 'sha': '14ea42490a753d0445fda074e8d3eac878d2aeed', 'workflow': 'test_pullrequest'}

 ******** GITHUB_WOKFLOW : https://github.com/arita37/mlmodels/actions?query=workflow%3Atest_pullrequest

 ******** GITHUB_REPO_BRANCH : https://github.com/arita37/mlmodels/tree/dev/

 ******** GITHUB_REPO_URL : https://github.com/arita37/mlmodels/tree/14ea42490a753d0445fda074e8d3eac878d2aeed

 ******** GITHUB_COMMIT_URL : https://github.com/arita37/mlmodels/commit/14ea42490a753d0445fda074e8d3eac878d2aeed
Package                   Version    Location
------------------------- ---------- -----------------------------------
absl-py                   0.9.0
alembic                   1.4.2
appdirs                   1.4.4
astor                     0.8.1
attrs                     19.3.0
autogluon                 0.0.5
bcrypt                    3.1.7
black                     19.10b0
blis                      0.4.1
boto                      2.49.0
boto3                     1.9.187
botocore                  1.12.253
catalogue                 1.0.0
catboost                  0.23.1
certifi                   2020.4.5.1
cffi                      1.14.0
chardet                   3.0.4
cli-code                  28.1.0
click                     7.1.2
cliff                     3.1.0
cloudpickle               1.4.1
cmd2                      0.8.9
cmdstanpy                 0.4.0
colorlog                  4.1.0
configparser              5.0.0
ConfigSpace               0.4.10
convertdate               2.2.1
cryptography              2.9.2
cycler                    0.10.0
cymem                     2.0.3
Cython                    0.29.17
dask                      2.6.0
databricks-cli            0.10.0
dataclasses               0.7
decorator                 4.4.2
deepctr                   0.7.4
dill                      0.3.1.1
distributed               2.6.0
docker                    4.2.0
docutils                  0.15.2
entrypoints               0.3
ephem                     3.7.7.1
fbprophet                 0.6
Flask                     1.1.2
future                    0.18.2
gast                      0.2.2
gensim                    3.8.3
gitdb                     4.0.5
GitPython                 3.1.2
gluoncv                   0.7.0
gluonnlp                  0.8.1
gluonts                   0.4.2
google-pasta              0.2.0
googleapis-common-protos  1.51.0
gorilla                   0.3.0
graphviz                  0.8.4
grpcio                    1.29.0
gunicorn                  20.0.4
h5py                      2.10.0
HeapDict                  1.0.1
holidays                  0.10.2
hyperopt                  0.1.2
idna                      2.9
importlib-metadata        1.6.0
itsdangerous              1.1.0
Jinja2                    2.11.2
jmespath                  0.10.0
joblib                    0.15.1
Keras                     2.3.1
Keras-Applications        1.0.8
keras-contrib             2.0.8
keras-mdn-layer           0.2.1
Keras-Preprocessing       1.1.2
kiwisolver                1.2.0
korean-lunar-calendar     0.2.1
lightgbm                  2.3.0
LunarCalendar             0.0.9
Mako                      1.1.2
Markdown                  3.2.2
MarkupSafe                1.1.1
matchzoo-py               1.1.1
matplotlib                3.2.1
mlflow                    1.7.1
mlmodels                  0.35.2     /home/runner/work/mlmodels/mlmodels
msgpack                   1.0.0
murmurhash                1.0.2
mxnet                     1.6.0
networkx                  2.4
nltk                      3.5
numexpr                   2.7.1
numpy                     1.18.2
opt-einsum                3.2.1
optuna                    1.1.0
pandas                    0.25.3
paramiko                  2.7.1
pathspec                  0.8.0
pbr                       5.4.5
Pillow                    6.2.2
pip                       20.1
plac                      1.1.3
plotly                    4.7.1
portalocker               1.7.0
preshed                   3.0.2
prettytable               0.7.2
prometheus-client         0.7.1
prometheus-flask-exporter 0.13.0
promise                   2.3
protobuf                  3.12.0
psutil                    5.7.0
pyaml                     20.4.0
pycparser                 2.20
pydantic                  1.4
PyMeeus                   0.3.7
pymongo                   3.10.1
PyNaCl                    1.3.0
pyparsing                 2.4.7
pyperclip                 1.8.0
pystan                    2.19.1.1
python-dateutil           2.8.0
python-editor             1.0.4
pytorch-lightning         0.7.3
pytorch-transformers      1.2.0
pytz                      2020.1
PyYAML                    5.3.1
querystring-parser        1.2.4
regex                     2020.5.14
requests                  2.23.0
retrying                  1.3.3
s3transfer                0.2.1
sacremoses                0.0.43
scikit-learn              0.21.2
scikit-optimize           0.7.4
scipy                     1.4.1
sentence-transformers     0.2.4
sentencepiece             0.1.90
setuptools                45.2.0
setuptools-git            1.2
simplejson                3.17.0
six                       1.14.0
smart-open                2.0.0
smmap                     3.0.4
sortedcontainers          2.1.0
spacy                     2.2.4
SQLAlchemy                1.3.13
sqlparse                  0.3.1
srsly                     1.0.2
stevedore                 1.32.0
tabulate                  0.8.7
tblib                     1.6.0
tensorboard               1.15.0
tensorboardX              2.0
tensorflow                1.15.2
tensorflow-datasets       3.0.0
tensorflow-estimator      1.15.1
tensorflow-metadata       0.22.0
tensorflow-probability    0.7.0
termcolor                 1.1.0
thinc                     7.4.0
toml                      0.10.1
toolz                     0.10.0
torch                     1.2.0
torchtext                 0.6.0
torchvision               0.4.0
tornado                   6.0.4
tqdm                      4.46.0
transformers              2.3.0
typed-ast                 1.4.1
typing                    3.7.4.1
ujson                     1.35
urllib3                   1.25.9
versioneer                0.18
wasabi                    0.6.0
wcwidth                   0.1.9
websocket-client          0.57.0
Werkzeug                  1.0.1
wheel                     0.34.2
wrapt                     1.12.1
zict                      2.0.0
zipp                      3.1.0

 ************************************************************************************************************************

 ************************************************************************************************************************

  /home/runner/work/mlmodels/mlmodels/pullrequest/ 

  ############Check model ################################ 

  ['/home/runner/work/mlmodels/mlmodels/pullrequest/aa_mycode_test.py'] 

  Used ['/home/runner/work/mlmodels/mlmodels/pullrequest/aa_mycode_test.py'] 

  ########### Run Check ############################## 





 ************************************************************************************************************************

 ******** TAG ::  {'github_repo_url': 'https://github.com/arita37/mlmodels/tree/14ea42490a753d0445fda074e8d3eac878d2aeed', 'url_branch_file': 'https://github.com/arita37/mlmodels/blob/dev/', 'repo': 'arita37/mlmodels', 'branch': 'dev', 'sha': '14ea42490a753d0445fda074e8d3eac878d2aeed', 'workflow': 'test_pullrequest'}

 ******** GITHUB_WOKFLOW : https://github.com/arita37/mlmodels/actions?query=workflow%3Atest_pullrequest

 ******** GITHUB_REPO_BRANCH : https://github.com/arita37/mlmodels/tree/dev/

 ******** GITHUB_REPO_URL : https://github.com/arita37/mlmodels/tree/14ea42490a753d0445fda074e8d3eac878d2aeed

 ******** GITHUB_COMMIT_URL : https://github.com/arita37/mlmodels/commit/14ea42490a753d0445fda074e8d3eac878d2aeed
Package                   Version    Location
------------------------- ---------- -----------------------------------
absl-py                   0.9.0
alembic                   1.4.2
appdirs                   1.4.4
astor                     0.8.1
attrs                     19.3.0
autogluon                 0.0.5
bcrypt                    3.1.7
black                     19.10b0
blis                      0.4.1
boto                      2.49.0
boto3                     1.9.187
botocore                  1.12.253
catalogue                 1.0.0
catboost                  0.23.1
certifi                   2020.4.5.1
cffi                      1.14.0
chardet                   3.0.4
cli-code                  28.1.0
click                     7.1.2
cliff                     3.1.0
cloudpickle               1.4.1
cmd2                      0.8.9
cmdstanpy                 0.4.0
colorlog                  4.1.0
configparser              5.0.0
ConfigSpace               0.4.10
convertdate               2.2.1
cryptography              2.9.2
cycler                    0.10.0
cymem                     2.0.3
Cython                    0.29.17
dask                      2.6.0
databricks-cli            0.10.0
dataclasses               0.7
decorator                 4.4.2
deepctr                   0.7.4
dill                      0.3.1.1
distributed               2.6.0
docker                    4.2.0
docutils                  0.15.2
entrypoints               0.3
ephem                     3.7.7.1
fbprophet                 0.6
Flask                     1.1.2
future                    0.18.2
gast                      0.2.2
gensim                    3.8.3
gitdb                     4.0.5
GitPython                 3.1.2
gluoncv                   0.7.0
gluonnlp                  0.8.1
gluonts                   0.4.2
google-pasta              0.2.0
googleapis-common-protos  1.51.0
gorilla                   0.3.0
graphviz                  0.8.4
grpcio                    1.29.0
gunicorn                  20.0.4
h5py                      2.10.0
HeapDict                  1.0.1
holidays                  0.10.2
hyperopt                  0.1.2
idna                      2.9
importlib-metadata        1.6.0
itsdangerous              1.1.0
Jinja2                    2.11.2
jmespath                  0.10.0
joblib                    0.15.1
Keras                     2.3.1
Keras-Applications        1.0.8
keras-contrib             2.0.8
keras-mdn-layer           0.2.1
Keras-Preprocessing       1.1.2
kiwisolver                1.2.0
korean-lunar-calendar     0.2.1
lightgbm                  2.3.0
LunarCalendar             0.0.9
Mako                      1.1.2
Markdown                  3.2.2
MarkupSafe                1.1.1
matchzoo-py               1.1.1
matplotlib                3.2.1
mlflow                    1.7.1
mlmodels                  0.35.2     /home/runner/work/mlmodels/mlmodels
msgpack                   1.0.0
murmurhash                1.0.2
mxnet                     1.6.0
networkx                  2.4
nltk                      3.5
numexpr                   2.7.1
numpy                     1.18.2
opt-einsum                3.2.1
optuna                    1.1.0
pandas                    0.25.3
paramiko                  2.7.1
pathspec                  0.8.0
pbr                       5.4.5
Pillow                    6.2.2
pip                       20.1
plac                      1.1.3
plotly                    4.7.1
portalocker               1.7.0
preshed                   3.0.2
prettytable               0.7.2
prometheus-client         0.7.1
prometheus-flask-exporter 0.13.0
promise                   2.3
protobuf                  3.12.0
psutil                    5.7.0
pyaml                     20.4.0
pycparser                 2.20
pydantic                  1.4
PyMeeus                   0.3.7
pymongo                   3.10.1
PyNaCl                    1.3.0
pyparsing                 2.4.7
pyperclip                 1.8.0
pystan                    2.19.1.1
python-dateutil           2.8.0
python-editor             1.0.4
pytorch-lightning         0.7.3
pytorch-transformers      1.2.0
pytz                      2020.1
PyYAML                    5.3.1
querystring-parser        1.2.4
regex                     2020.5.14
requests                  2.23.0
retrying                  1.3.3
s3transfer                0.2.1
sacremoses                0.0.43
scikit-learn              0.21.2
scikit-optimize           0.7.4
scipy                     1.4.1
sentence-transformers     0.2.4
sentencepiece             0.1.90
setuptools                45.2.0
setuptools-git            1.2
simplejson                3.17.0
six                       1.14.0
smart-open                2.0.0
smmap                     3.0.4
sortedcontainers          2.1.0
spacy                     2.2.4
SQLAlchemy                1.3.13
sqlparse                  0.3.1
srsly                     1.0.2
stevedore                 1.32.0
tabulate                  0.8.7
tblib                     1.6.0
tensorboard               1.15.0
tensorboardX              2.0
tensorflow                1.15.2
tensorflow-datasets       3.0.0
tensorflow-estimator      1.15.1
tensorflow-metadata       0.22.0
tensorflow-probability    0.7.0
termcolor                 1.1.0
thinc                     7.4.0
toml                      0.10.1
toolz                     0.10.0
torch                     1.2.0
torchtext                 0.6.0
torchvision               0.4.0
tornado                   6.0.4
tqdm                      4.46.0
transformers              2.3.0
typed-ast                 1.4.1
typing                    3.7.4.1
ujson                     1.35
urllib3                   1.25.9
versioneer                0.18
wasabi                    0.6.0
wcwidth                   0.1.9
websocket-client          0.57.0
Werkzeug                  1.0.1
wheel                     0.34.2
wrapt                     1.12.1
zict                      2.0.0
zipp                      3.1.0

 ************************************************************************************************************************

 ************************************************************************************************************************





 ************************************************************************************************************************

  test_import 
['model_flow.__init__', 'model_keras.keras_gan', 'model_keras.textcnn_dataloader', 'model_keras.preprocess', 'model_keras.nbeats', 'model_keras.01_deepctr', 'model_keras.textvae', 'model_keras.namentity_crm_bilstm_dataloader', 'model_keras.Autokeras', 'model_keras.util', 'model_keras.charcnn_zhang', 'model_keras.charcnn', 'model_keras.__init__', 'model_keras.namentity_crm_bilstm', 'model_keras.textcnn', 'model_keras.armdn', 'model_keras.02_cnn', 'model_dev.__init__', 'model_tf.1_lstm', 'model_tf.util', 'model_tf.__init__', 'model_tf.temporal_fusion_google', 'model_gluon.util', 'model_gluon.gluon_automl', 'model_gluon.util_autogluon', 'model_gluon.fb_prophet', 'model_gluon.__init__', 'model_gluon.gluonts_model', 'model_sklearn.model_sklearn', 'model_sklearn.model_lightgbm', 'model_sklearn.__init__', 'example.vision_mnist', 'example.benchmark_timeseries_m4', 'example.arun_hyper', 'example.lightgbm_glass', 'example.benchmark_timeseries_m5', 'example.arun_model', 'utils.ztest_structure', 'utils.test_dataloader', 'utils.parse', 'model_tch.util_transformer', 'model_tch.nbeats', 'model_tch.transformer_classifier', 'model_tch.matchzoo_models', 'model_tch.util_data', 'model_tch.torchhub', 'model_tch.03_nbeats_dataloader', 'model_tch.__init__', 'model_tch.transformer_sentence', 'model_tch.pytorch_vae', 'model_tch.pplm', 'model_tch.textcnn', 'model_tch.mlp', 'template.model_xxx', 'template.00_template_keras', 'preprocess.tabular_keras', 'preprocess.image', 'preprocess.timeseries', 'preprocess.ztemp', 'preprocess.text_torch', 'preprocess.text_keras', 'preprocess.text', 'preprocess.tabular', 'preprocess.generic_old', 'preprocess.__init__', 'preprocess.generic', 'model_rank.__init__']
mlmodels.model_flow.__init__

  Error mlmodels.model_keras.keras_gan module 'mlmodels.model_keras.raw.keras_gan' has no attribute 'aae' 
mlmodels.model_keras.textcnn_dataloader
mlmodels.model_keras.preprocess
mlmodels.model_keras.nbeats
mlmodels.model_keras.01_deepctr
mlmodels.model_keras.textvae
mlmodels.model_keras.namentity_crm_bilstm_dataloader

  Error mlmodels.model_keras.Autokeras No module named 'autokeras' 
Using TensorFlow backend.
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
mlmodels.model_keras.util
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset
mlmodels.model_keras.charcnn_zhang
mlmodels.model_keras.charcnn
mlmodels.model_keras.__init__
mlmodels.model_keras.namentity_crm_bilstm
mlmodels.model_keras.textcnn
mlmodels.model_keras.armdn
mlmodels.model_keras.02_cnn
mlmodels.model_dev.__init__
mlmodels.model_tf.1_lstm
mlmodels.model_tf.util
mlmodels.model_tf.__init__

  Error mlmodels.model_tf.temporal_fusion_google No module named 'mlmodels.mode_tf' 
/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB
  Optimizer.opt_registry[name].__name__))
/home/runner/work/mlmodels/mlmodels/mlmodels/model_gluon/gluonts_model.py:569: DeprecationWarning:

invalid escape sequence \s

Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
mlmodels.model_gluon.util
mlmodels.model_gluon.gluon_automl
mlmodels.model_gluon.util_autogluon
mlmodels.model_gluon.fb_prophet
mlmodels.model_gluon.__init__
mlmodels.model_gluon.gluonts_model
mlmodels.model_sklearn.model_sklearn
mlmodels.model_sklearn.model_lightgbm
mlmodels.model_sklearn.__init__

  Error mlmodels.example.vision_mnist invalid syntax (vision_mnist.py, line 15) 
mlmodels.example.benchmark_timeseries_m4

  Error mlmodels.example.arun_hyper name 'mlmodels' is not defined 
Deprecaton set to False
/home/runner/work/mlmodels/mlmodels

  Error mlmodels.example.lightgbm_glass [Errno 2] No such file or directory: 'lightgbm_glass.json' 

  Error mlmodels.example.benchmark_timeseries_m5 [Errno 2] File b'./m5-forecasting-accuracy/calendar.csv' does not exist: b'./m5-forecasting-accuracy/calendar.csv' 
<module 'mlmodels' from '/home/runner/work/mlmodels/mlmodels/mlmodels/__init__.py'>
/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/ardmn.json

  Error mlmodels.example.arun_model [Errno 2] No such file or directory: '/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/ardmn.json' 
mlmodels.utils.ztest_structure
mlmodels.utils.test_dataloader
mlmodels.utils.parse
mlmodels.model_tch.util_transformer
mlmodels.model_tch.nbeats

  Error mlmodels.model_tch.transformer_classifier No module named 'util_transformer' 
mlmodels.model_tch.matchzoo_models

  Error mlmodels.model_tch.util_data [Errno 2] File b'./data/train.csv' does not exist: b'./data/train.csv' 
mlmodels.model_tch.torchhub

  Error mlmodels.model_tch.03_nbeats_dataloader No module named 'dataloader' 
/home/runner/work/mlmodels/mlmodels/mlmodels/model_sklearn/model_sklearn.py:1187: DeprecationWarning:

invalid escape sequence \*

PyTorch version 1.2.0 available.
mlmodels.model_tch.__init__
mlmodels.model_tch.transformer_sentence

  Error mlmodels.model_tchtorch_vae No module named 'mlmodels.model_tchtorch_vae' 
mlmodels.model_tch.pplm
mlmodels.model_tch.textcnn
mlmodels.model_tch.mlp

  Error mlmodels.template.model_xxx name '__file___' is not defined 

  Error mlmodels.template.00_template_keras expected an indented block (00_template_keras.py, line 68) 
mlmodels.preprocess.tabular_keras
mlmodels.preprocess.image
mlmodels.preprocess.timeseries

  Error mlmodels.preprocess.ztemp invalid character in identifier (ztemp.py, line 6) 
Deprecaton set to False

  {'model_uri': 'model_tf.1_lstm', 'learning_rate': 0.001, 'num_layers': 1, 'size': 6, 'size_layer': 128, 'output_size': 6, 'timestep': 4, 'epoch': 2} {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} {'engine': 'optuna', 'method': 'prune', 'ntrials': 5} {'engine_pars': {'engine': 'optuna', 'method': 'normal', 'ntrials': 2, 'metric_target': 'loss'}, 'learning_rate': {'type': 'log_uniform', 'init': 0.01, 'range': [0.001, 0.1]}, 'num_layers': {'type': 'int', 'init': 2, 'range': [2, 4]}, 'size': {'type': 'int', 'init': 6, 'range': [6, 6]}, 'output_size': {'type': 'int', 'init': 6, 'range': [6, 6]}, 'size_layer': {'type': 'categorical', 'value': [128, 256]}, 'timestep': {'type': 'categorical', 'value': [5]}, 'epoch': {'type': 'categorical', 'value': [2]}} 

  <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> 

  ###### Hyper-optimization through study   ################################## 

  check <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} 
[32m[I 2020-05-16 13:10:48,489][0m Finished trial#0 resulted in value: 0.29199040681123734. Current best value is 0.29199040681123734 with parameters: {'learning_rate': 0.004458764145223537, 'num_layers': 4, 'size': 6, 'output_size': 6, 'size_layer': 128, 'timestep': 5, 'epoch': 2}.[0m
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  check <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} 
[32m[I 2020-05-16 13:10:50,607][0m Finished trial#1 resulted in value: 12.04047441482544. Current best value is 0.29199040681123734 with parameters: {'learning_rate': 0.004458764145223537, 'num_layers': 4, 'size': 6, 'output_size': 6, 'size_layer': 128, 'timestep': 5, 'epoch': 2}.[0m
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

 ################################### Optim, finished ###################################

  ### Save Stats   ########################################################## 

  ### Run Model with best   ################################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Saving     ########################################################### 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/optim_1lstm/', 'model_type': 'model_tf', 'model_uri': 'model_tf-1_lstm'}
Model saved in path: /home/runner/work/mlmodels/mlmodels/mlmodels/ztest/optim_1lstm//model//model.ckpt
sh: 1: ml_mlmodels: not found
mlmodels.preprocess.text_torch
mlmodels.preprocess.text_keras
mlmodels.preprocess.text
mlmodels.preprocess.tabular
mlmodels.preprocess.generic_old
mlmodels.preprocess.__init__
mlmodels.preprocess.generic
mlmodels.model_rank.__init__





 ************************************************************************************************************************

  python /home/runner/work/mlmodels/mlmodels/pullrequest/aa_mycode_test.py  2>&1 | tee -a  cd log_.txt 
os.getcwd /home/runner/work/mlmodels/mlmodels
############ Your custom code ################################



 python /home/runner/work/mlmodels/mlmodels/mlmodels/optim.py  
Deprecaton set to False

  {'model_uri': 'model_tf.1_lstm', 'learning_rate': 0.001, 'num_layers': 1, 'size': 6, 'size_layer': 128, 'output_size': 6, 'timestep': 4, 'epoch': 2} {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} {'engine': 'optuna', 'method': 'prune', 'ntrials': 5} {'engine_pars': {'engine': 'optuna', 'method': 'normal', 'ntrials': 2, 'metric_target': 'loss'}, 'learning_rate': {'type': 'log_uniform', 'init': 0.01, 'range': [0.001, 0.1]}, 'num_layers': {'type': 'int', 'init': 2, 'range': [2, 4]}, 'size': {'type': 'int', 'init': 6, 'range': [6, 6]}, 'output_size': {'type': 'int', 'init': 6, 'range': [6, 6]}, 'size_layer': {'type': 'categorical', 'value': [128, 256]}, 'timestep': {'type': 'categorical', 'value': [5]}, 'epoch': {'type': 'categorical', 'value': [2]}} 

  <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> 

  ###### Hyper-optimization through study   ################################## 

  check <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} 

  <mlmodels.model_tf.1_lstm.Model object at 0x7f419bd91588> 
[32m[I 2020-05-16 13:10:58,349][0m Finished trial#0 resulted in value: 4.043760061264038. Current best value is 4.043760061264038 with parameters: {'learning_rate': 0.03341249951431583, 'num_layers': 4, 'size': 6, 'output_size': 6, 'size_layer': 256, 'timestep': 5, 'epoch': 2}.[0m
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  check <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} 

  <mlmodels.model_tf.1_lstm.Model object at 0x7f419b84ecf8> 
[32m[I 2020-05-16 13:10:59,926][0m Finished trial#1 resulted in value: 0.28548790514469147. Current best value is 0.28548790514469147 with parameters: {'learning_rate': 0.0029528360547771823, 'num_layers': 3, 'size': 6, 'output_size': 6, 'size_layer': 128, 'timestep': 5, 'epoch': 2}.[0m
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

 ################################### Optim, finished ###################################

  ### Save Stats   ########################################################## 

  ### Run Model with best   ################################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Saving     ########################################################### 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/optim_1lstm/', 'model_type': 'model_tf', 'model_uri': 'model_tf-1_lstm'}
Model saved in path: /home/runner/work/mlmodels/mlmodels/mlmodels/ztest/optim_1lstm//model//model.ckpt



 python /home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/textcnn.py    

  #### Loading params   ############################################## 

  #### Path params   ########################################## 

  #### Loading dataset   ############################################# 
Loading data...
Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz

    8192/17464789 [..............................] - ETA: 0s
 2818048/17464789 [===>..........................] - ETA: 0s
11714560/17464789 [===================>..........] - ETA: 0s
16769024/17464789 [===========================>..] - ETA: 0s
17465344/17464789 [==============================] - 0s 0us/step
Pad sequences (samples x time)...

  #### Model init, fit   ############################################# 
Using TensorFlow backend.
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 40)           0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 40, 50)       250         input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 38, 128)      19328       embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 37, 128)      25728       embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 36, 128)      32128       embedding_1[0][0]                
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_2 (GlobalM (None, 128)          0           conv1d_2[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_3 (GlobalM (None, 128)          0           conv1d_3[0][0]                   
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_1[0][0]     
                                                                 global_max_pooling1d_2[0][0]     
                                                                 global_max_pooling1d_3[0][0]     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            385         concatenate_1[0][0]              
==================================================================================================
Total params: 77,819
Trainable params: 77,819
Non-trainable params: 0
__________________________________________________________________________________________________
Loading data...
Pad sequences (samples x time)...
Train on 25000 samples, validate on 25000 samples
Epoch 1/1

 1000/25000 [>.............................] - ETA: 11s - loss: 7.6973 - accuracy: 0.4980
 2000/25000 [=>............................] - ETA: 8s - loss: 7.6513 - accuracy: 0.5010 
 3000/25000 [==>...........................] - ETA: 6s - loss: 7.6615 - accuracy: 0.5003
 4000/25000 [===>..........................] - ETA: 5s - loss: 7.6705 - accuracy: 0.4997
 5000/25000 [=====>........................] - ETA: 5s - loss: 7.7188 - accuracy: 0.4966
 6000/25000 [======>.......................] - ETA: 4s - loss: 7.6615 - accuracy: 0.5003
 7000/25000 [=======>......................] - ETA: 4s - loss: 7.6863 - accuracy: 0.4987
 8000/25000 [========>.....................] - ETA: 4s - loss: 7.6494 - accuracy: 0.5011
 9000/25000 [=========>....................] - ETA: 3s - loss: 7.6257 - accuracy: 0.5027
10000/25000 [===========>..................] - ETA: 3s - loss: 7.6145 - accuracy: 0.5034
11000/25000 [============>.................] - ETA: 3s - loss: 7.6387 - accuracy: 0.5018
12000/25000 [=============>................] - ETA: 3s - loss: 7.6551 - accuracy: 0.5008
13000/25000 [==============>...............] - ETA: 2s - loss: 7.6631 - accuracy: 0.5002
14000/25000 [===============>..............] - ETA: 2s - loss: 7.6579 - accuracy: 0.5006
15000/25000 [=================>............] - ETA: 2s - loss: 7.6544 - accuracy: 0.5008
16000/25000 [==================>...........] - ETA: 2s - loss: 7.6800 - accuracy: 0.4991
17000/25000 [===================>..........] - ETA: 1s - loss: 7.6910 - accuracy: 0.4984
18000/25000 [====================>.........] - ETA: 1s - loss: 7.6862 - accuracy: 0.4987
19000/25000 [=====================>........] - ETA: 1s - loss: 7.6707 - accuracy: 0.4997
20000/25000 [=======================>......] - ETA: 1s - loss: 7.6666 - accuracy: 0.5000
21000/25000 [========================>.....] - ETA: 0s - loss: 7.6542 - accuracy: 0.5008
22000/25000 [=========================>....] - ETA: 0s - loss: 7.6610 - accuracy: 0.5004
23000/25000 [==========================>...] - ETA: 0s - loss: 7.6653 - accuracy: 0.5001
24000/25000 [===========================>..] - ETA: 0s - loss: 7.6692 - accuracy: 0.4998
25000/25000 [==============================] - 7s 272us/step - loss: 7.6666 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000

  #### save the trained model  ####################################### 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5'}

  #### Predict   ##################################################### 
Loading data...

  #### metrics   ##################################################### 
{}

  #### Plot   ######################################################## 

  #### Save/Load   ################################################### 
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5'}
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5'}
(<mlmodels.util.Model_empty object at 0x7fc8532df780>, None)

  #### Module init   ############################################ 

  <module 'mlmodels.model_keras.textcnn' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/textcnn.py'> 

  #### Loading params   ############################################## 

  #### Path params   ########################################## 

  #### Model init   ############################################ 
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 40)           0                                            
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 40, 50)       250         input_2[0][0]                    
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 38, 128)      19328       embedding_2[0][0]                
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 37, 128)      25728       embedding_2[0][0]                
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 36, 128)      32128       embedding_2[0][0]                
__________________________________________________________________________________________________
global_max_pooling1d_4 (GlobalM (None, 128)          0           conv1d_4[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_5 (GlobalM (None, 128)          0           conv1d_5[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_6 (GlobalM (None, 128)          0           conv1d_6[0][0]                   
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 384)          0           global_max_pooling1d_4[0][0]     
                                                                 global_max_pooling1d_5[0][0]     
                                                                 global_max_pooling1d_6[0][0]     
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            385         concatenate_2[0][0]              
==================================================================================================
Total params: 77,819
Trainable params: 77,819
Non-trainable params: 0
__________________________________________________________________________________________________

  <mlmodels.model_keras.textcnn.Model object at 0x7fc86c5eaeb8> 

  #### Fit   ######################################################## 
Loading data...
Pad sequences (samples x time)...
Train on 25000 samples, validate on 25000 samples
Epoch 1/1

 1000/25000 [>.............................] - ETA: 11s - loss: 7.3293 - accuracy: 0.5220
 2000/25000 [=>............................] - ETA: 7s - loss: 7.1146 - accuracy: 0.5360 
 3000/25000 [==>...........................] - ETA: 6s - loss: 7.2680 - accuracy: 0.5260
 4000/25000 [===>..........................] - ETA: 6s - loss: 7.5401 - accuracy: 0.5082
 5000/25000 [=====>........................] - ETA: 5s - loss: 7.5348 - accuracy: 0.5086
 6000/25000 [======>.......................] - ETA: 5s - loss: 7.4647 - accuracy: 0.5132
 7000/25000 [=======>......................] - ETA: 4s - loss: 7.5045 - accuracy: 0.5106
 8000/25000 [========>.....................] - ETA: 4s - loss: 7.5344 - accuracy: 0.5086
 9000/25000 [=========>....................] - ETA: 4s - loss: 7.5542 - accuracy: 0.5073
10000/25000 [===========>..................] - ETA: 3s - loss: 7.5532 - accuracy: 0.5074
11000/25000 [============>.................] - ETA: 3s - loss: 7.5746 - accuracy: 0.5060
12000/25000 [=============>................] - ETA: 3s - loss: 7.6015 - accuracy: 0.5042
13000/25000 [==============>...............] - ETA: 2s - loss: 7.5923 - accuracy: 0.5048
14000/25000 [===============>..............] - ETA: 2s - loss: 7.5659 - accuracy: 0.5066
15000/25000 [=================>............] - ETA: 2s - loss: 7.5593 - accuracy: 0.5070
16000/25000 [==================>...........] - ETA: 2s - loss: 7.5631 - accuracy: 0.5067
17000/25000 [===================>..........] - ETA: 1s - loss: 7.5755 - accuracy: 0.5059
18000/25000 [====================>.........] - ETA: 1s - loss: 7.5729 - accuracy: 0.5061
19000/25000 [=====================>........] - ETA: 1s - loss: 7.5811 - accuracy: 0.5056
20000/25000 [=======================>......] - ETA: 1s - loss: 7.5915 - accuracy: 0.5049
21000/25000 [========================>.....] - ETA: 0s - loss: 7.6250 - accuracy: 0.5027
22000/25000 [=========================>....] - ETA: 0s - loss: 7.6401 - accuracy: 0.5017
23000/25000 [==========================>...] - ETA: 0s - loss: 7.6466 - accuracy: 0.5013
24000/25000 [===========================>..] - ETA: 0s - loss: 7.6570 - accuracy: 0.5006
25000/25000 [==============================] - 7s 284us/step - loss: 7.6666 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000

  #### Predict   #################################################### 
Loading data...
(array([[1.],
       [1.],
       [1.],
       ...,
       [1.],
       [1.],
       [1.]], dtype=float32), None)

  #### Get  metrics   ################################################ 

  #### Save   ######################################################## 

  #### Load   ######################################################## 

  ############ Model preparation   ################################## 

  #### Module init   ############################################ 

  <module 'mlmodels.model_keras.textcnn' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/textcnn.py'> 

  #### Loading params   ############################################## 

  #### Path params   ########################################## 

  #### Model init   ############################################ 
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 40)           0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 40, 50)       250         input_3[0][0]                    
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 38, 128)      19328       embedding_3[0][0]                
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 37, 128)      25728       embedding_3[0][0]                
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 36, 128)      32128       embedding_3[0][0]                
__________________________________________________________________________________________________
global_max_pooling1d_7 (GlobalM (None, 128)          0           conv1d_7[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_8 (GlobalM (None, 128)          0           conv1d_8[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_9 (GlobalM (None, 128)          0           conv1d_9[0][0]                   
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 384)          0           global_max_pooling1d_7[0][0]     
                                                                 global_max_pooling1d_8[0][0]     
                                                                 global_max_pooling1d_9[0][0]     
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            385         concatenate_3[0][0]              
==================================================================================================
Total params: 77,819
Trainable params: 77,819
Non-trainable params: 0
__________________________________________________________________________________________________

  ############ Model fit   ########################################## 
Loading data...
Pad sequences (samples x time)...
Train on 25000 samples, validate on 25000 samples
Epoch 1/1

 1000/25000 [>.............................] - ETA: 11s - loss: 7.2680 - accuracy: 0.5260
 2000/25000 [=>............................] - ETA: 8s - loss: 7.4366 - accuracy: 0.5150 
 3000/25000 [==>...........................] - ETA: 6s - loss: 7.6206 - accuracy: 0.5030
 4000/25000 [===>..........................] - ETA: 6s - loss: 7.7318 - accuracy: 0.4958
 5000/25000 [=====>........................] - ETA: 5s - loss: 7.7004 - accuracy: 0.4978
 6000/25000 [======>.......................] - ETA: 5s - loss: 7.7203 - accuracy: 0.4965
 7000/25000 [=======>......................] - ETA: 4s - loss: 7.6973 - accuracy: 0.4980
 8000/25000 [========>.....................] - ETA: 4s - loss: 7.6820 - accuracy: 0.4990
 9000/25000 [=========>....................] - ETA: 4s - loss: 7.6462 - accuracy: 0.5013
10000/25000 [===========>..................] - ETA: 3s - loss: 7.6804 - accuracy: 0.4991
11000/25000 [============>.................] - ETA: 3s - loss: 7.6499 - accuracy: 0.5011
12000/25000 [=============>................] - ETA: 3s - loss: 7.6653 - accuracy: 0.5001
13000/25000 [==============>...............] - ETA: 2s - loss: 7.6419 - accuracy: 0.5016
14000/25000 [===============>..............] - ETA: 2s - loss: 7.6425 - accuracy: 0.5016
15000/25000 [=================>............] - ETA: 2s - loss: 7.6482 - accuracy: 0.5012
16000/25000 [==================>...........] - ETA: 2s - loss: 7.6187 - accuracy: 0.5031
17000/25000 [===================>..........] - ETA: 1s - loss: 7.6215 - accuracy: 0.5029
18000/25000 [====================>.........] - ETA: 1s - loss: 7.6206 - accuracy: 0.5030
19000/25000 [=====================>........] - ETA: 1s - loss: 7.6319 - accuracy: 0.5023
20000/25000 [=======================>......] - ETA: 1s - loss: 7.6551 - accuracy: 0.5008
21000/25000 [========================>.....] - ETA: 0s - loss: 7.6440 - accuracy: 0.5015
22000/25000 [=========================>....] - ETA: 0s - loss: 7.6604 - accuracy: 0.5004
23000/25000 [==========================>...] - ETA: 0s - loss: 7.6606 - accuracy: 0.5004
24000/25000 [===========================>..] - ETA: 0s - loss: 7.6692 - accuracy: 0.4998
25000/25000 [==============================] - 7s 278us/step - loss: 7.6666 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000
fit success None

  ############ Prediction############################################ 
Loading data...
(array([[1.],
       [1.],
       [1.],
       ...,
       [1.],
       [1.],
       [1.]], dtype=float32), None)

  ############ Save/ Load ############################################ 
