
  test_pullrequest /home/runner/work/mlmodels/mlmodels/mlmodels/config/test_config.json Namespace(config_file='/home/runner/work/mlmodels/mlmodels/mlmodels/config/test_config.json', config_mode='test', do='test_pullrequest', folder=None, log_file=None, save_folder='ztest/') 

  ml_test --do test_pullrequest 





 ************************************************************************************************************************

 ******** TAG ::  {'github_repo_url': 'https://github.com/arita37/mlmodels/tree/69b309ad857428cc5a734b8afd99842edf9b2a42', 'url_branch_file': 'https://github.com/arita37/mlmodels/blob/dev/', 'repo': 'arita37/mlmodels', 'branch': 'dev', 'sha': '69b309ad857428cc5a734b8afd99842edf9b2a42', 'workflow': 'test_pullrequest'}

 ******** GITHUB_WOKFLOW : https://github.com/arita37/mlmodels/actions?query=workflow%3Atest_pullrequest

 ******** GITHUB_REPO_BRANCH : https://github.com/arita37/mlmodels/tree/dev/

 ******** GITHUB_REPO_URL : https://github.com/arita37/mlmodels/tree/69b309ad857428cc5a734b8afd99842edf9b2a42

 ******** GITHUB_COMMIT_URL : https://github.com/arita37/mlmodels/commit/69b309ad857428cc5a734b8afd99842edf9b2a42

 ******** Click here for Online DEBUGGER : https://gitpod.io/#https://github.com/arita37/mlmodels/tree/69b309ad857428cc5a734b8afd99842edf9b2a42

 ************************************************************************************************************************

  /home/runner/work/mlmodels/mlmodels/pullrequest/ 

  ############Check model ################################ 

  ['/home/runner/work/mlmodels/mlmodels/pullrequest/aa_mycode_test.py'] 

  Used ['/home/runner/work/mlmodels/mlmodels/pullrequest/aa_mycode_test.py'] 

  ########### Run Check ############################## 





 ************************************************************************************************************************

 ******** TAG ::  {'github_repo_url': 'https://github.com/arita37/mlmodels/tree/69b309ad857428cc5a734b8afd99842edf9b2a42', 'url_branch_file': 'https://github.com/arita37/mlmodels/blob/dev/', 'repo': 'arita37/mlmodels', 'branch': 'dev', 'sha': '69b309ad857428cc5a734b8afd99842edf9b2a42', 'workflow': 'test_pullrequest'}

 ******** GITHUB_WOKFLOW : https://github.com/arita37/mlmodels/actions?query=workflow%3Atest_pullrequest

 ******** GITHUB_REPO_BRANCH : https://github.com/arita37/mlmodels/tree/dev/

 ******** GITHUB_REPO_URL : https://github.com/arita37/mlmodels/tree/69b309ad857428cc5a734b8afd99842edf9b2a42

 ******** GITHUB_COMMIT_URL : https://github.com/arita37/mlmodels/commit/69b309ad857428cc5a734b8afd99842edf9b2a42

 ******** Click here for Online DEBUGGER : https://gitpod.io/#https://github.com/arita37/mlmodels/tree/69b309ad857428cc5a734b8afd99842edf9b2a42

 ************************************************************************************************************************





 ************************************************************************************************************************

  test_import 
['model_flow.__init__', 'model_keras.keras_gan', 'model_keras.preprocess', 'model_keras.nbeats', 'model_keras.01_deepctr', 'model_keras.textvae', 'model_keras.namentity_crm_bilstm_dataloader', 'model_keras.Autokeras', 'model_keras.util', 'model_keras.charcnn_zhang', 'model_keras.charcnn', 'model_keras.__init__', 'model_keras.namentity_crm_bilstm', 'model_keras.textcnn', 'model_keras.armdn', 'model_keras.02_cnn', 'model_dev.__init__', 'model_tf.1_lstm', 'model_tf.util', 'model_tf.__init__', 'model_tf.temporal_fusion_google', 'model_gluon.util', 'model_gluon.gluon_automl', 'model_gluon.util_autogluon', 'model_gluon.fb_prophet', 'model_gluon.__init__', 'model_gluon.gluonts_model', 'model_sklearn.model_sklearn', 'model_sklearn.model_lightgbm', 'model_sklearn.__init__', 'example.vision_mnist', 'example.benchmark_timeseries_m4', 'example.arun_hyper', 'example.lightgbm_glass', 'example.benchmark_timeseries_m5', 'example.arun_model', 'utils.ztest_structure', 'utils.test_dataloader', 'utils.parse', 'model_tch.util_transformer', 'model_tch.nbeats', 'model_tch.transformer_classifier', 'model_tch.matchzoo_models', 'model_tch.util_data', 'model_tch.torchhub', 'model_tch.03_nbeats_dataloader', 'model_tch.__init__', 'model_tch.transformer_sentence', 'model_tch.pytorch_vae', 'model_tch.pplm', 'model_tch.textcnn', 'model_tch.mlp', 'template.model_xxx', 'template.00_template_keras', 'preprocess.tabular_keras', 'preprocess.image', 'preprocess.timeseries', 'preprocess.ztemp', 'preprocess.text_torch', 'preprocess.text_keras', 'preprocess.text', 'preprocess.tabular', 'preprocess.generic_old', 'preprocess.__init__', 'preprocess.generic', 'model_rank.__init__']
mlmodels.model_flow.__init__

  Error mlmodels.model_keras.keras_gan module 'mlmodels.model_keras.raw.keras_gan' has no attribute 'aae' 
mlmodels.model_keras.preprocess
mlmodels.model_keras.nbeats
mlmodels.model_keras.01_deepctr
mlmodels.model_keras.textvae
mlmodels.model_keras.namentity_crm_bilstm_dataloader

  Error mlmodels.model_keras.Autokeras No module named 'autokeras' 
Using TensorFlow backend.
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
mlmodels.model_keras.util
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset
mlmodels.model_keras.charcnn_zhang
mlmodels.model_keras.charcnn
mlmodels.model_keras.__init__
mlmodels.model_keras.namentity_crm_bilstm
mlmodels.model_keras.textcnn
mlmodels.model_keras.armdn
mlmodels.model_keras.02_cnn
mlmodels.model_dev.__init__
mlmodels.model_tf.1_lstm
mlmodels.model_tf.util
mlmodels.model_tf.__init__

  Error mlmodels.model_tf.temporal_fusion_google No module named 'mlmodels.mode_tf' 
/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB
  Optimizer.opt_registry[name].__name__))
/home/runner/work/mlmodels/mlmodels/mlmodels/model_gluon/gluonts_model.py:569: DeprecationWarning:

invalid escape sequence \s

Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
Using CPU
mlmodels.model_gluon.util
mlmodels.model_gluon.gluon_automl
mlmodels.model_gluon.util_autogluon
mlmodels.model_gluon.fb_prophet
mlmodels.model_gluon.__init__
mlmodels.model_gluon.gluonts_model
mlmodels.model_sklearn.model_sklearn
mlmodels.model_sklearn.model_lightgbm
mlmodels.model_sklearn.__init__

  Error mlmodels.example.vision_mnist invalid syntax (vision_mnist.py, line 15) 
mlmodels.example.benchmark_timeseries_m4

  Error mlmodels.example.arun_hyper name 'mlmodels' is not defined 
Deprecaton set to False
/home/runner/work/mlmodels/mlmodels

  Error mlmodels.example.lightgbm_glass [Errno 2] No such file or directory: 'lightgbm_glass.json' 

  Error mlmodels.example.benchmark_timeseries_m5 [Errno 2] File b'./m5-forecasting-accuracy/calendar.csv' does not exist: b'./m5-forecasting-accuracy/calendar.csv' 
<module 'mlmodels' from '/home/runner/work/mlmodels/mlmodels/mlmodels/__init__.py'>
/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/ardmn.json

  Error mlmodels.example.arun_model [Errno 2] No such file or directory: '/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/ardmn.json' 
mlmodels.utils.ztest_structure
mlmodels.utils.test_dataloader
mlmodels.utils.parse
mlmodels.model_tch.util_transformer
mlmodels.model_tch.nbeats

  Error mlmodels.model_tch.transformer_classifier No module named 'util_transformer' 
mlmodels.model_tch.matchzoo_models

  Error mlmodels.model_tch.util_data [Errno 2] File b'./data/train.csv' does not exist: b'./data/train.csv' 
mlmodels.model_tch.torchhub

  Error mlmodels.model_tch.03_nbeats_dataloader No module named 'dataloader' 
/home/runner/work/mlmodels/mlmodels/mlmodels/model_sklearn/model_sklearn.py:1187: DeprecationWarning:

invalid escape sequence \*

PyTorch version 1.2.0 available.
mlmodels.model_tch.__init__
mlmodels.model_tch.transformer_sentence

  Error mlmodels.model_tchtorch_vae No module named 'mlmodels.model_tchtorch_vae' 
mlmodels.model_tch.pplm
mlmodels.model_tch.textcnn
mlmodels.model_tch.mlp

  Error mlmodels.template.model_xxx name '__file___' is not defined 

  Error mlmodels.template.00_template_keras expected an indented block (00_template_keras.py, line 68) 
mlmodels.preprocess.tabular_keras
mlmodels.preprocess.image
mlmodels.preprocess.timeseries

  Error mlmodels.preprocess.ztemp invalid character in identifier (ztemp.py, line 6) 
Deprecaton set to False

  {'model_uri': 'model_tf.1_lstm', 'learning_rate': 0.001, 'num_layers': 1, 'size': 6, 'size_layer': 128, 'output_size': 6, 'timestep': 4, 'epoch': 2} {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} {'engine': 'optuna', 'method': 'prune', 'ntrials': 5} {'engine_pars': {'engine': 'optuna', 'method': 'normal', 'ntrials': 2, 'metric_target': 'loss'}, 'learning_rate': {'type': 'log_uniform', 'init': 0.01, 'range': [0.001, 0.1]}, 'num_layers': {'type': 'int', 'init': 2, 'range': [2, 4]}, 'size': {'type': 'int', 'init': 6, 'range': [6, 6]}, 'output_size': {'type': 'int', 'init': 6, 'range': [6, 6]}, 'size_layer': {'type': 'categorical', 'value': [128, 256]}, 'timestep': {'type': 'categorical', 'value': [5]}, 'epoch': {'type': 'categorical', 'value': [2]}} 

  <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> 

  ###### Hyper-optimization through study   ################################## 

  check <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} 
[32m[I 2020-05-20 11:10:35,213][0m Finished trial#0 resulted in value: 0.9654667675495148. Current best value is 0.9654667675495148 with parameters: {'learning_rate': 0.006158925365181705, 'num_layers': 4, 'size': 6, 'output_size': 6, 'size_layer': 256, 'timestep': 5, 'epoch': 2}.[0m
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  check <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} 
[32m[I 2020-05-20 11:10:36,704][0m Finished trial#1 resulted in value: 0.31026720255613327. Current best value is 0.31026720255613327 with parameters: {'learning_rate': 0.0026174131326978066, 'num_layers': 3, 'size': 6, 'output_size': 6, 'size_layer': 128, 'timestep': 5, 'epoch': 2}.[0m
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

 ################################### Optim, finished ###################################

  ### Save Stats   ########################################################## 

  ### Run Model with best   ################################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Saving     ########################################################### 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/optim_1lstm/', 'model_type': 'model_tf', 'model_uri': 'model_tf-1_lstm'}
Model saved in path: /home/runner/work/mlmodels/mlmodels/mlmodels/ztest/optim_1lstm//model//model.ckpt
sh: 1: ml_mlmodels: not found
mlmodels.preprocess.text_torch
mlmodels.preprocess.text_keras
mlmodels.preprocess.text
mlmodels.preprocess.tabular
mlmodels.preprocess.generic_old
mlmodels.preprocess.__init__
mlmodels.preprocess.generic
mlmodels.model_rank.__init__





 ************************************************************************************************************************

  python /home/runner/work/mlmodels/mlmodels/pullrequest/aa_mycode_test.py  2>&1 | tee -a  cd log_.txt 
os.getcwd /home/runner/work/mlmodels/mlmodels
############ Your custom code ################################



 python /home/runner/work/mlmodels/mlmodels/mlmodels/optim.py  
Deprecaton set to False

  {'model_uri': 'model_tf.1_lstm', 'learning_rate': 0.001, 'num_layers': 1, 'size': 6, 'size_layer': 128, 'output_size': 6, 'timestep': 4, 'epoch': 2} {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} {'engine': 'optuna', 'method': 'prune', 'ntrials': 5} {'engine_pars': {'engine': 'optuna', 'method': 'normal', 'ntrials': 2, 'metric_target': 'loss'}, 'learning_rate': {'type': 'log_uniform', 'init': 0.01, 'range': [0.001, 0.1]}, 'num_layers': {'type': 'int', 'init': 2, 'range': [2, 4]}, 'size': {'type': 'int', 'init': 6, 'range': [6, 6]}, 'output_size': {'type': 'int', 'init': 6, 'range': [6, 6]}, 'size_layer': {'type': 'categorical', 'value': [128, 256]}, 'timestep': {'type': 'categorical', 'value': [5]}, 'epoch': {'type': 'categorical', 'value': [2]}} 

  <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> 

  ###### Hyper-optimization through study   ################################## 

  check <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} 

  <mlmodels.model_tf.1_lstm.Model object at 0x7ff1013f93c8> 
[32m[I 2020-05-20 11:10:42,782][0m Finished trial#0 resulted in value: 0.33545053005218506. Current best value is 0.33545053005218506 with parameters: {'learning_rate': 0.0017198333978411878, 'num_layers': 2, 'size': 6, 'output_size': 6, 'size_layer': 128, 'timestep': 5, 'epoch': 2}.[0m
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  check <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]} 

  <mlmodels.model_tf.1_lstm.Model object at 0x7ff119796470> 
[32m[I 2020-05-20 11:10:44,797][0m Finished trial#1 resulted in value: 0.7750603705644608. Current best value is 0.33545053005218506 with parameters: {'learning_rate': 0.0017198333978411878, 'num_layers': 2, 'size': 6, 'output_size': 6, 'size_layer': 128, 'timestep': 5, 'epoch': 2}.[0m
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

 ################################### Optim, finished ###################################

  ### Save Stats   ########################################################## 

  ### Run Model with best   ################################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year_small.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Saving     ########################################################### 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/optim_1lstm/', 'model_type': 'model_tf', 'model_uri': 'model_tf-1_lstm'}
Model saved in path: /home/runner/work/mlmodels/mlmodels/mlmodels/ztest/optim_1lstm//model//model.ckpt



 python /home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/textcnn.py    

  #### Loading params   ############################################## 

  #### Path params   ########################################## 

  #### Loading dataset   ############################################# 
Loading data...
Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz

    8192/17464789 [..............................] - ETA: 0s
   24576/17464789 [..............................] - ETA: 48s
   57344/17464789 [..............................] - ETA: 41s
   73728/17464789 [..............................] - ETA: 48s
  106496/17464789 [..............................] - ETA: 44s
  139264/17464789 [..............................] - ETA: 42s
  180224/17464789 [..............................] - ETA: 39s
  212992/17464789 [..............................] - ETA: 38s
  245760/17464789 [..............................] - ETA: 38s
  278528/17464789 [..............................] - ETA: 37s
  319488/17464789 [..............................] - ETA: 36s
  352256/17464789 [..............................] - ETA: 36s
  385024/17464789 [..............................] - ETA: 36s
  417792/17464789 [..............................] - ETA: 36s
  458752/17464789 [..............................] - ETA: 37s
  491520/17464789 [..............................] - ETA: 36s
  524288/17464789 [..............................] - ETA: 36s
  557056/17464789 [..............................] - ETA: 36s
  598016/17464789 [>.............................] - ETA: 35s
  647168/17464789 [>.............................] - ETA: 34s
  679936/17464789 [>.............................] - ETA: 34s
  720896/17464789 [>.............................] - ETA: 34s
  770048/17464789 [>.............................] - ETA: 33s
  802816/17464789 [>.............................] - ETA: 33s
  860160/17464789 [>.............................] - ETA: 32s
  892928/17464789 [>.............................] - ETA: 32s
  942080/17464789 [>.............................] - ETA: 31s
  974848/17464789 [>.............................] - ETA: 31s
 1032192/17464789 [>.............................] - ETA: 31s
 1081344/17464789 [>.............................] - ETA: 30s
 1114112/17464789 [>.............................] - ETA: 30s
 1171456/17464789 [=>............................] - ETA: 30s
 1220608/17464789 [=>............................] - ETA: 29s
 1277952/17464789 [=>............................] - ETA: 29s
 1310720/17464789 [=>............................] - ETA: 29s
 1359872/17464789 [=>............................] - ETA: 29s
 1417216/17464789 [=>............................] - ETA: 28s
 1466368/17464789 [=>............................] - ETA: 28s
 1515520/17464789 [=>............................] - ETA: 27s
 1572864/17464789 [=>............................] - ETA: 27s
 1622016/17464789 [=>............................] - ETA: 27s
 1687552/17464789 [=>............................] - ETA: 26s
 1744896/17464789 [=>............................] - ETA: 26s
 1794048/17464789 [==>...........................] - ETA: 26s
 1851392/17464789 [==>...........................] - ETA: 25s
 1900544/17464789 [==>...........................] - ETA: 25s
 1974272/17464789 [==>...........................] - ETA: 25s
 2023424/17464789 [==>...........................] - ETA: 24s
 2088960/17464789 [==>...........................] - ETA: 24s
 2146304/17464789 [==>...........................] - ETA: 24s
 2211840/17464789 [==>...........................] - ETA: 23s
 2269184/17464789 [==>...........................] - ETA: 23s
 2334720/17464789 [===>..........................] - ETA: 23s
 2392064/17464789 [===>..........................] - ETA: 23s
 2424832/17464789 [===>..........................] - ETA: 23s
 2473984/17464789 [===>..........................] - ETA: 23s
 2506752/17464789 [===>..........................] - ETA: 23s
 2564096/17464789 [===>..........................] - ETA: 22s
 2613248/17464789 [===>..........................] - ETA: 22s
 2670592/17464789 [===>..........................] - ETA: 22s
 2703360/17464789 [===>..........................] - ETA: 22s
 2752512/17464789 [===>..........................] - ETA: 22s
 2801664/17464789 [===>..........................] - ETA: 22s
 2842624/17464789 [===>..........................] - ETA: 22s
 2891776/17464789 [===>..........................] - ETA: 22s
 2949120/17464789 [====>.........................] - ETA: 22s
 2998272/17464789 [====>.........................] - ETA: 22s
 3047424/17464789 [====>.........................] - ETA: 21s
 3104768/17464789 [====>.........................] - ETA: 21s
 3153920/17464789 [====>.........................] - ETA: 21s
 3186688/17464789 [====>.........................] - ETA: 21s
 3244032/17464789 [====>.........................] - ETA: 21s
 3293184/17464789 [====>.........................] - ETA: 21s
 3342336/17464789 [====>.........................] - ETA: 21s
 3399680/17464789 [====>.........................] - ETA: 21s
 3448832/17464789 [====>.........................] - ETA: 21s
 3506176/17464789 [=====>........................] - ETA: 21s
 3555328/17464789 [=====>........................] - ETA: 20s
 3604480/17464789 [=====>........................] - ETA: 20s
 3661824/17464789 [=====>........................] - ETA: 20s
 3710976/17464789 [=====>........................] - ETA: 20s
 3743744/17464789 [=====>........................] - ETA: 20s
 3801088/17464789 [=====>........................] - ETA: 20s
 3850240/17464789 [=====>........................] - ETA: 20s
 3899392/17464789 [=====>........................] - ETA: 20s
 3956736/17464789 [=====>........................] - ETA: 20s
 4005888/17464789 [=====>........................] - ETA: 20s
 4038656/17464789 [=====>........................] - ETA: 20s
 4096000/17464789 [======>.......................] - ETA: 19s
 4145152/17464789 [======>.......................] - ETA: 19s
 4202496/17464789 [======>.......................] - ETA: 19s
 4251648/17464789 [======>.......................] - ETA: 19s
 4300800/17464789 [======>.......................] - ETA: 19s
 4358144/17464789 [======>.......................] - ETA: 19s
 4407296/17464789 [======>.......................] - ETA: 19s
 4440064/17464789 [======>.......................] - ETA: 19s
 4497408/17464789 [======>.......................] - ETA: 19s
 4546560/17464789 [======>.......................] - ETA: 19s
 4595712/17464789 [======>.......................] - ETA: 19s
 4653056/17464789 [======>.......................] - ETA: 18s
 4702208/17464789 [=======>......................] - ETA: 18s
 4759552/17464789 [=======>......................] - ETA: 18s
 4808704/17464789 [=======>......................] - ETA: 18s
 4857856/17464789 [=======>......................] - ETA: 18s
 4915200/17464789 [=======>......................] - ETA: 18s
 4964352/17464789 [=======>......................] - ETA: 18s
 5013504/17464789 [=======>......................] - ETA: 18s
 5070848/17464789 [=======>......................] - ETA: 18s
 5136384/17464789 [=======>......................] - ETA: 17s
 5193728/17464789 [=======>......................] - ETA: 17s
 5242880/17464789 [========>.....................] - ETA: 17s
 5316608/17464789 [========>.....................] - ETA: 17s
 5365760/17464789 [========>.....................] - ETA: 17s
 5414912/17464789 [========>.....................] - ETA: 17s
 5488640/17464789 [========>.....................] - ETA: 17s
 5537792/17464789 [========>.....................] - ETA: 17s
 5611520/17464789 [========>.....................] - ETA: 16s
 5677056/17464789 [========>.....................] - ETA: 16s
 5734400/17464789 [========>.....................] - ETA: 16s
 5799936/17464789 [========>.....................] - ETA: 16s
 5873664/17464789 [=========>....................] - ETA: 16s
 5922816/17464789 [=========>....................] - ETA: 16s
 5988352/17464789 [=========>....................] - ETA: 16s
 6062080/17464789 [=========>....................] - ETA: 16s
 6127616/17464789 [=========>....................] - ETA: 15s
 6201344/17464789 [=========>....................] - ETA: 15s
 6266880/17464789 [=========>....................] - ETA: 15s
 6340608/17464789 [=========>....................] - ETA: 15s
 6406144/17464789 [==========>...................] - ETA: 15s
 6496256/17464789 [==========>...................] - ETA: 15s
 6569984/17464789 [==========>...................] - ETA: 14s
 6619136/17464789 [==========>...................] - ETA: 14s
 6668288/17464789 [==========>...................] - ETA: 14s
 6725632/17464789 [==========>...................] - ETA: 14s
 6774784/17464789 [==========>...................] - ETA: 14s
 6823936/17464789 [==========>...................] - ETA: 14s
 6897664/17464789 [==========>...................] - ETA: 14s
 6946816/17464789 [==========>...................] - ETA: 14s
 7004160/17464789 [===========>..................] - ETA: 14s
 7069696/17464789 [===========>..................] - ETA: 14s
 7143424/17464789 [===========>..................] - ETA: 13s
 7192576/17464789 [===========>..................] - ETA: 13s
 7266304/17464789 [===========>..................] - ETA: 13s
 7331840/17464789 [===========>..................] - ETA: 13s
 7380992/17464789 [===========>..................] - ETA: 13s
 7454720/17464789 [===========>..................] - ETA: 13s
 7520256/17464789 [===========>..................] - ETA: 13s
 7593984/17464789 [============>.................] - ETA: 13s
 7659520/17464789 [============>.................] - ETA: 13s
 7733248/17464789 [============>.................] - ETA: 12s
 7798784/17464789 [============>.................] - ETA: 12s
 7872512/17464789 [============>.................] - ETA: 12s
 7938048/17464789 [============>.................] - ETA: 12s
 8011776/17464789 [============>.................] - ETA: 12s
 8077312/17464789 [============>.................] - ETA: 12s
 8151040/17464789 [=============>................] - ETA: 12s
 8183808/17464789 [=============>................] - ETA: 12s
 8273920/17464789 [=============>................] - ETA: 12s
 8339456/17464789 [=============>................] - ETA: 11s
 8396800/17464789 [=============>................] - ETA: 11s
 8429568/17464789 [=============>................] - ETA: 11s
 8495104/17464789 [=============>................] - ETA: 11s
 8552448/17464789 [=============>................] - ETA: 11s
 8601600/17464789 [=============>................] - ETA: 11s
 8658944/17464789 [=============>................] - ETA: 11s
 8724480/17464789 [=============>................] - ETA: 11s
 8773632/17464789 [==============>...............] - ETA: 11s
 8847360/17464789 [==============>...............] - ETA: 11s
 8896512/17464789 [==============>...............] - ETA: 11s
 8970240/17464789 [==============>...............] - ETA: 11s
 9019392/17464789 [==============>...............] - ETA: 10s
 9093120/17464789 [==============>...............] - ETA: 10s
 9158656/17464789 [==============>...............] - ETA: 10s
 9216000/17464789 [==============>...............] - ETA: 10s
 9281536/17464789 [==============>...............] - ETA: 10s
 9355264/17464789 [===============>..............] - ETA: 10s
 9420800/17464789 [===============>..............] - ETA: 10s
 9494528/17464789 [===============>..............] - ETA: 10s
 9560064/17464789 [===============>..............] - ETA: 10s
 9633792/17464789 [===============>..............] - ETA: 10s
 9699328/17464789 [===============>..............] - ETA: 9s 
 9773056/17464789 [===============>..............] - ETA: 9s
 9838592/17464789 [===============>..............] - ETA: 9s
 9912320/17464789 [================>.............] - ETA: 9s
 9977856/17464789 [================>.............] - ETA: 9s
10051584/17464789 [================>.............] - ETA: 9s
10117120/17464789 [================>.............] - ETA: 9s
10190848/17464789 [================>.............] - ETA: 9s
10256384/17464789 [================>.............] - ETA: 9s
10346496/17464789 [================>.............] - ETA: 8s
10412032/17464789 [================>.............] - ETA: 8s
10485760/17464789 [=================>............] - ETA: 8s
10551296/17464789 [=================>............] - ETA: 8s
10625024/17464789 [=================>............] - ETA: 8s
10690560/17464789 [=================>............] - ETA: 8s
10780672/17464789 [=================>............] - ETA: 8s
10846208/17464789 [=================>............] - ETA: 8s
10919936/17464789 [=================>............] - ETA: 8s
11001856/17464789 [=================>............] - ETA: 8s
11075584/17464789 [==================>...........] - ETA: 7s
11141120/17464789 [==================>...........] - ETA: 7s
11231232/17464789 [==================>...........] - ETA: 7s
11304960/17464789 [==================>...........] - ETA: 7s
11386880/17464789 [==================>...........] - ETA: 7s
11476992/17464789 [==================>...........] - ETA: 7s
11542528/17464789 [==================>...........] - ETA: 7s
11632640/17464789 [==================>...........] - ETA: 7s
11722752/17464789 [===================>..........] - ETA: 6s
11804672/17464789 [===================>..........] - ETA: 6s
11878400/17464789 [===================>..........] - ETA: 6s
11960320/17464789 [===================>..........] - ETA: 6s
12050432/17464789 [===================>..........] - ETA: 6s
12140544/17464789 [===================>..........] - ETA: 6s
12222464/17464789 [===================>..........] - ETA: 6s
12312576/17464789 [====================>.........] - ETA: 6s
12386304/17464789 [====================>.........] - ETA: 6s
12468224/17464789 [====================>.........] - ETA: 5s
12558336/17464789 [====================>.........] - ETA: 5s
12656640/17464789 [====================>.........] - ETA: 5s
12746752/17464789 [====================>.........] - ETA: 5s
12836864/17464789 [=====================>........] - ETA: 5s
12918784/17464789 [=====================>........] - ETA: 5s
13025280/17464789 [=====================>........] - ETA: 5s
13115392/17464789 [=====================>........] - ETA: 5s
13213696/17464789 [=====================>........] - ETA: 4s
13303808/17464789 [=====================>........] - ETA: 4s
13410304/17464789 [======================>.......] - ETA: 4s
13492224/17464789 [======================>.......] - ETA: 4s
13598720/17464789 [======================>.......] - ETA: 4s
13705216/17464789 [======================>.......] - ETA: 4s
13787136/17464789 [======================>.......] - ETA: 4s
13893632/17464789 [======================>.......] - ETA: 4s
14000128/17464789 [=======================>......] - ETA: 3s
14106624/17464789 [=======================>......] - ETA: 3s
14204928/17464789 [=======================>......] - ETA: 3s
14295040/17464789 [=======================>......] - ETA: 3s
14401536/17464789 [=======================>......] - ETA: 3s
14508032/17464789 [=======================>......] - ETA: 3s
14606336/17464789 [========================>.....] - ETA: 3s
14729216/17464789 [========================>.....] - ETA: 3s
14835712/17464789 [========================>.....] - ETA: 2s
14942208/17464789 [========================>.....] - ETA: 2s
15040512/17464789 [========================>.....] - ETA: 2s
15147008/17464789 [=========================>....] - ETA: 2s
15269888/17464789 [=========================>....] - ETA: 2s
15376384/17464789 [=========================>....] - ETA: 2s
15482880/17464789 [=========================>....] - ETA: 2s
15597568/17464789 [=========================>....] - ETA: 2s
15704064/17464789 [=========================>....] - ETA: 1s
15826944/17464789 [==========================>...] - ETA: 1s
15933440/17464789 [==========================>...] - ETA: 1s
16056320/17464789 [==========================>...] - ETA: 1s
16154624/17464789 [==========================>...] - ETA: 1s
16277504/17464789 [==========================>...] - ETA: 1s
16400384/17464789 [===========================>..] - ETA: 1s
16523264/17464789 [===========================>..] - ETA: 0s
16629760/17464789 [===========================>..] - ETA: 0s
16752640/17464789 [===========================>..] - ETA: 0s
16875520/17464789 [===========================>..] - ETA: 0s
16990208/17464789 [============================>.] - ETA: 0s
17113088/17464789 [============================>.] - ETA: 0s
17235968/17464789 [============================>.] - ETA: 0s
17358848/17464789 [============================>.] - ETA: 0s
17465344/17464789 [==============================] - 18s 1us/step
Pad sequences (samples x time)...

  #### Model init, fit   ############################################# 
Using TensorFlow backend.
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 40)           0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 40, 50)       250         input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 38, 128)      19328       embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 37, 128)      25728       embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 36, 128)      32128       embedding_1[0][0]                
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_2 (GlobalM (None, 128)          0           conv1d_2[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_3 (GlobalM (None, 128)          0           conv1d_3[0][0]                   
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_1[0][0]     
                                                                 global_max_pooling1d_2[0][0]     
                                                                 global_max_pooling1d_3[0][0]     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            385         concatenate_1[0][0]              
==================================================================================================
Total params: 77,819
Trainable params: 77,819
Non-trainable params: 0
__________________________________________________________________________________________________
Loading data...
Pad sequences (samples x time)...
Train on 25000 samples, validate on 25000 samples
Epoch 1/1

 1000/25000 [>.............................] - ETA: 11s - loss: 7.9273 - accuracy: 0.4830
 2000/25000 [=>............................] - ETA: 7s - loss: 7.8813 - accuracy: 0.4860 
 3000/25000 [==>...........................] - ETA: 6s - loss: 7.7280 - accuracy: 0.4960
 4000/25000 [===>..........................] - ETA: 5s - loss: 7.7050 - accuracy: 0.4975
 5000/25000 [=====>........................] - ETA: 5s - loss: 7.6513 - accuracy: 0.5010
 6000/25000 [======>.......................] - ETA: 4s - loss: 7.6820 - accuracy: 0.4990
 7000/25000 [=======>......................] - ETA: 4s - loss: 7.6162 - accuracy: 0.5033
 8000/25000 [========>.....................] - ETA: 4s - loss: 7.6206 - accuracy: 0.5030
 9000/25000 [=========>....................] - ETA: 3s - loss: 7.5610 - accuracy: 0.5069
10000/25000 [===========>..................] - ETA: 3s - loss: 7.5562 - accuracy: 0.5072
11000/25000 [============>.................] - ETA: 3s - loss: 7.5760 - accuracy: 0.5059
12000/25000 [=============>................] - ETA: 3s - loss: 7.6219 - accuracy: 0.5029
13000/25000 [==============>...............] - ETA: 2s - loss: 7.6171 - accuracy: 0.5032
14000/25000 [===============>..............] - ETA: 2s - loss: 7.6151 - accuracy: 0.5034
15000/25000 [=================>............] - ETA: 2s - loss: 7.6196 - accuracy: 0.5031
16000/25000 [==================>...........] - ETA: 2s - loss: 7.6331 - accuracy: 0.5022
17000/25000 [===================>..........] - ETA: 1s - loss: 7.6585 - accuracy: 0.5005
18000/25000 [====================>.........] - ETA: 1s - loss: 7.6556 - accuracy: 0.5007
19000/25000 [=====================>........] - ETA: 1s - loss: 7.6690 - accuracy: 0.4998
20000/25000 [=======================>......] - ETA: 1s - loss: 7.6590 - accuracy: 0.5005
21000/25000 [========================>.....] - ETA: 0s - loss: 7.6601 - accuracy: 0.5004
22000/25000 [=========================>....] - ETA: 0s - loss: 7.6673 - accuracy: 0.5000
23000/25000 [==========================>...] - ETA: 0s - loss: 7.6640 - accuracy: 0.5002
24000/25000 [===========================>..] - ETA: 0s - loss: 7.6564 - accuracy: 0.5007
25000/25000 [==============================] - 7s 267us/step - loss: 7.6666 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000

  #### save the trained model  ####################################### 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5'}

  #### Predict   ##################################################### 
Loading data...

  #### metrics   ##################################################### 
{}

  #### Plot   ######################################################## 

  #### Save/Load   ################################################### 
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5'}
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_keras/textcnn/model.h5'}
(<mlmodels.util.Model_empty object at 0x7fe2dc79d630>, None)

  #### Module init   ############################################ 

  <module 'mlmodels.model_keras.textcnn' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/textcnn.py'> 

  #### Loading params   ############################################## 

  #### Path params   ########################################## 

  #### Model init   ############################################ 
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 40)           0                                            
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 40, 50)       250         input_2[0][0]                    
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 38, 128)      19328       embedding_2[0][0]                
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 37, 128)      25728       embedding_2[0][0]                
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 36, 128)      32128       embedding_2[0][0]                
__________________________________________________________________________________________________
global_max_pooling1d_4 (GlobalM (None, 128)          0           conv1d_4[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_5 (GlobalM (None, 128)          0           conv1d_5[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_6 (GlobalM (None, 128)          0           conv1d_6[0][0]                   
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 384)          0           global_max_pooling1d_4[0][0]     
                                                                 global_max_pooling1d_5[0][0]     
                                                                 global_max_pooling1d_6[0][0]     
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            385         concatenate_2[0][0]              
==================================================================================================
Total params: 77,819
Trainable params: 77,819
Non-trainable params: 0
__________________________________________________________________________________________________

  <mlmodels.model_keras.textcnn.Model object at 0x7fe2dff4a7b8> 

  #### Fit   ######################################################## 
Loading data...
Pad sequences (samples x time)...
Train on 25000 samples, validate on 25000 samples
Epoch 1/1

 1000/25000 [>.............................] - ETA: 10s - loss: 7.5286 - accuracy: 0.5090
 2000/25000 [=>............................] - ETA: 7s - loss: 7.5056 - accuracy: 0.5105 
 3000/25000 [==>...........................] - ETA: 6s - loss: 7.5133 - accuracy: 0.5100
 4000/25000 [===>..........................] - ETA: 5s - loss: 7.5325 - accuracy: 0.5088
 5000/25000 [=====>........................] - ETA: 5s - loss: 7.5593 - accuracy: 0.5070
 6000/25000 [======>.......................] - ETA: 4s - loss: 7.6053 - accuracy: 0.5040
 7000/25000 [=======>......................] - ETA: 4s - loss: 7.5768 - accuracy: 0.5059
 8000/25000 [========>.....................] - ETA: 4s - loss: 7.5900 - accuracy: 0.5050
 9000/25000 [=========>....................] - ETA: 3s - loss: 7.6053 - accuracy: 0.5040
10000/25000 [===========>..................] - ETA: 3s - loss: 7.5992 - accuracy: 0.5044
11000/25000 [============>.................] - ETA: 3s - loss: 7.6318 - accuracy: 0.5023
12000/25000 [=============>................] - ETA: 2s - loss: 7.6155 - accuracy: 0.5033
13000/25000 [==============>...............] - ETA: 2s - loss: 7.6041 - accuracy: 0.5041
14000/25000 [===============>..............] - ETA: 2s - loss: 7.6086 - accuracy: 0.5038
15000/25000 [=================>............] - ETA: 2s - loss: 7.6247 - accuracy: 0.5027
16000/25000 [==================>...........] - ETA: 2s - loss: 7.6379 - accuracy: 0.5019
17000/25000 [===================>..........] - ETA: 1s - loss: 7.6576 - accuracy: 0.5006
18000/25000 [====================>.........] - ETA: 1s - loss: 7.6581 - accuracy: 0.5006
19000/25000 [=====================>........] - ETA: 1s - loss: 7.6666 - accuracy: 0.5000
20000/25000 [=======================>......] - ETA: 1s - loss: 7.6498 - accuracy: 0.5011
21000/25000 [========================>.....] - ETA: 0s - loss: 7.6491 - accuracy: 0.5011
22000/25000 [=========================>....] - ETA: 0s - loss: 7.6548 - accuracy: 0.5008
23000/25000 [==========================>...] - ETA: 0s - loss: 7.6613 - accuracy: 0.5003
24000/25000 [===========================>..] - ETA: 0s - loss: 7.6685 - accuracy: 0.4999
25000/25000 [==============================] - 7s 263us/step - loss: 7.6666 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000

  #### Predict   #################################################### 
Loading data...
(array([[1.],
       [1.],
       [1.],
       ...,
       [1.],
       [1.],
       [1.]], dtype=float32), None)

  #### Get  metrics   ################################################ 

  #### Save   ######################################################## 

  #### Load   ######################################################## 

  ############ Model preparation   ################################## 

  #### Module init   ############################################ 

  <module 'mlmodels.model_keras.textcnn' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/textcnn.py'> 

  #### Loading params   ############################################## 

  #### Path params   ########################################## 

  #### Model init   ############################################ 
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 40)           0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 40, 50)       250         input_3[0][0]                    
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 38, 128)      19328       embedding_3[0][0]                
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 37, 128)      25728       embedding_3[0][0]                
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 36, 128)      32128       embedding_3[0][0]                
__________________________________________________________________________________________________
global_max_pooling1d_7 (GlobalM (None, 128)          0           conv1d_7[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_8 (GlobalM (None, 128)          0           conv1d_8[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_9 (GlobalM (None, 128)          0           conv1d_9[0][0]                   
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 384)          0           global_max_pooling1d_7[0][0]     
                                                                 global_max_pooling1d_8[0][0]     
                                                                 global_max_pooling1d_9[0][0]     
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            385         concatenate_3[0][0]              
==================================================================================================
Total params: 77,819
Trainable params: 77,819
Non-trainable params: 0
__________________________________________________________________________________________________

  ############ Model fit   ########################################## 
Loading data...
Pad sequences (samples x time)...
Train on 25000 samples, validate on 25000 samples
Epoch 1/1

 1000/25000 [>.............................] - ETA: 11s - loss: 7.7433 - accuracy: 0.4950
 2000/25000 [=>............................] - ETA: 7s - loss: 7.7050 - accuracy: 0.4975 
 3000/25000 [==>...........................] - ETA: 6s - loss: 7.7433 - accuracy: 0.4950
 4000/25000 [===>..........................] - ETA: 5s - loss: 7.7241 - accuracy: 0.4963
 5000/25000 [=====>........................] - ETA: 5s - loss: 7.7004 - accuracy: 0.4978
 6000/25000 [======>.......................] - ETA: 4s - loss: 7.7075 - accuracy: 0.4973
 7000/25000 [=======>......................] - ETA: 4s - loss: 7.7170 - accuracy: 0.4967
 8000/25000 [========>.....................] - ETA: 4s - loss: 7.7107 - accuracy: 0.4971
 9000/25000 [=========>....................] - ETA: 3s - loss: 7.6649 - accuracy: 0.5001
10000/25000 [===========>..................] - ETA: 3s - loss: 7.6544 - accuracy: 0.5008
11000/25000 [============>.................] - ETA: 3s - loss: 7.6485 - accuracy: 0.5012
12000/25000 [=============>................] - ETA: 3s - loss: 7.6449 - accuracy: 0.5014
13000/25000 [==============>...............] - ETA: 2s - loss: 7.6312 - accuracy: 0.5023
14000/25000 [===============>..............] - ETA: 2s - loss: 7.6568 - accuracy: 0.5006
15000/25000 [=================>............] - ETA: 2s - loss: 7.6523 - accuracy: 0.5009
16000/25000 [==================>...........] - ETA: 2s - loss: 7.6551 - accuracy: 0.5008
17000/25000 [===================>..........] - ETA: 1s - loss: 7.6459 - accuracy: 0.5014
18000/25000 [====================>.........] - ETA: 1s - loss: 7.6573 - accuracy: 0.5006
19000/25000 [=====================>........] - ETA: 1s - loss: 7.6666 - accuracy: 0.5000
20000/25000 [=======================>......] - ETA: 1s - loss: 7.6919 - accuracy: 0.4983
21000/25000 [========================>.....] - ETA: 0s - loss: 7.6841 - accuracy: 0.4989
22000/25000 [=========================>....] - ETA: 0s - loss: 7.6882 - accuracy: 0.4986
23000/25000 [==========================>...] - ETA: 0s - loss: 7.6933 - accuracy: 0.4983
24000/25000 [===========================>..] - ETA: 0s - loss: 7.6730 - accuracy: 0.4996
25000/25000 [==============================] - 7s 270us/step - loss: 7.6666 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000
fit success None

  ############ Prediction############################################ 
Loading data...
(array([[1.],
       [1.],
       [1.],
       ...,
       [1.],
       [1.],
       [1.]], dtype=float32), None)

  ############ Save/ Load ############################################ 
